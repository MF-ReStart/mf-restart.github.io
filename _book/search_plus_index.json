{"./":{"url":"./","title":"Introduction","keywords":"","body":"README 所有你现在觉得理所当然的平稳运行，背后都有曾经为之崩溃过的人。 "},"worknote/kubernetes/readme.html":{"url":"worknote/kubernetes/readme.html","title":"Kubernetes","keywords":"","body":"Kubernetes 1.概述 2.部署Kubernetes集群 3.Kubelet概述 4.工作负载资源使用 5.Service资源 6.存储 7.配置 8.StatefulSet 9.认证授权与准入控制 "},"worknote/kubernetes/overview.html":{"url":"worknote/kubernetes/overview.html","title":"概述","keywords":"","body":"概述 [info] 说明 此文档仅为个人的学习笔记，记录下来是为了以后遗忘时可以翻阅。最好的 Kubernetes 文档在 kubernetes.io，请大家去官网学习！ Kubernetes 是什么？ Kubernetes 是一个可移植的、可扩展的开源平台，用于管理容器化的工作负载和服务，可促进声明式配置和自动化。Kubernetes 拥有一个庞大且快速增长的生态系统。Kubernetes 的服务、支持和工具广泛可用。 名称 Kubernetes 源于希腊语，意为“舵手”或“飞行员”。Google 在 2014 年开源了 Kubernetes 项目， Kubernetes 建立在 Google 在大规模运行生产工作负载方面拥有十几年的经验 的基础上，结合了社区中最好的想法和实践。 时光回溯 让我们回顾一下为什么 Kubernetes 如此有用。 传统部署时代： 早期，各个组织机构在物理服务器上运行应用程序。无法为物理服务器中的应用程序定义资源边界，这会导致资源分配问题。 例如，如果在物理服务器上运行多个应用程序，则可能会出现一个应用程序占用大部分资源的情况， 结果可能导致其他应用程序的性能下降。 一种解决方案是在不同的物理服务器上运行每个应用程序，但是由于资源利用不足而无法扩展， 并且维护许多物理服务器的成本很高。 虚拟化部署时代： 作为解决方案，引入了虚拟化。虚拟化技术允许你在单个物理服务器的 CPU 上运行多个虚拟机（VM）。 虚拟化允许应用程序在 VM 之间隔离，并提供一定程度的安全，因为一个应用程序的信息不能被另一应用程序随意访问。 虚拟化技术能够更好地利用物理服务器上的资源，并且因为可轻松地添加或更新应用程序而可以实现更好的可伸缩性，降低硬件成本等等。 每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。 容器部署时代： 容器类似于 VM，但是它们具有被放宽的隔离属性，可以在应用程序之间共享操作系统（OS）。 因此，容器被认为是轻量级的。容器与 VM 类似，具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。 容器因具有许多优势而变得流行起来。下面列出的是容器的一些好处： 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性），支持可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建/发布时而不是在部署时创建应用程序容器镜像， 从而将应用程序与基础架构分离。 可观察性不仅可以显示操作系统级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在便携式计算机上与在云中相同地运行。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。 Kubernetes 能做什么？ 容器是打包和运行应用程序的最好方式。在生产环境中，你需要管理运行应用程序的容器，并确保不会停机。 例如，如果一个容器发生故障，则需要启动另一个容器。如果由系统处理此行为，会不会更容易？ 这就是 Kubernetes 来解决这些问题的方法！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移、部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。 Kubernetes 为你提供： 服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。 存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。 自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态，它可以以受控的速率将实际状态更改为期望状态。例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。 自动完成装箱计算 Kubernetes 允许你指定每个容器所需 CPU 和内存（RAM）。 当容器指定了资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。 自我修复 Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。 密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。 Kubernetes 不是什么？ Kubernetes 不是传统的、包罗万象的 PaaS（平台即服务）系统。 由于 Kubernetes 在容器级别而不是在硬件级别运行，它提供了 PaaS 产品共有的一些普遍适用的功能， 例如部署、扩展、负载均衡、日志记录和监视。 但是，Kubernetes 不是单体系统，默认解决方案都是可选和可插拔的。 Kubernetes 提供了构建开发人员平台的基础，但是在重要的地方保留了用户的选择和灵活性。 Kubernetes： 不限制支持的应用程序类型。 Kubernetes 旨在支持极其多种多样的工作负载，包括无状态、有状态和数据处理工作负载。 如果应用程序可以在容器中运行，那么它应该可以在 Kubernetes 上很好地运行。 不部署源代码，也不构建你的应用程序。 持续集成(CI)、交付和部署（CI/CD）工作流取决于组织的文化和偏好以及技术要求。 不提供应用程序级别的服务作为内置服务，例如中间件（例如，消息中间件）、 数据处理框架（例如，Spark）、数据库（例如，mysql）、缓存、集群存储系统 （例如，Ceph）。这样的组件可以在 Kubernetes 上运行，并且/或者可以由运行在 Kubernetes 上的应用程序通过可移植机制（例如， 开放服务代理）来访问。 不要求日志记录、监视或警报解决方案。 它提供了一些集成作为概念证明，并提供了收集和导出指标的机制。 不提供或不要求配置语言/系统（例如 jsonnet），它提供了声明性 API， 该声明性 API 可以由任意形式的声明性规范所构成。 不提供也不采用任何全面的机器配置、维护、管理或自我修复系统。 此外，Kubernetes 不仅仅是一个编排系统，实际上它消除了编排的需要。 编排的技术定义是执行已定义的工作流程：首先执行 A，然后执行 B，再执行 C。 相比之下，Kubernetes 包含一组独立的、可组合的控制过程， 这些过程连续地将当前状态驱动到所提供的所需状态。 如何从 A 到 C 的方式无关紧要，也不需要集中控制，这使得系统更易于使用 且功能更强大、系统更健壮、更为弹性和可扩展。 Kubernetes 组件 当你部署完 Kubernetes, 即拥有了一个完整的集群。 一个 Kubernetes 集群由一组被称作节点的机器组成。这些节点上运行 Kubernetes 所管理的容器化应用。集群具有至少一个工作节点。 工作节点托管作为应用负载的组件的 Pod 。控制平面管理集群中的工作节点和 Pod 。 为集群提供故障转移和高可用性，这些控制平面一般跨多主机运行，集群跨多个节点运行。 本文档概述了交付正常运行的 Kubernetes 集群所需的各种组件。 控制平面组件（Control Plane Components） 控制平面的组件对集群做出全局决策（比如调度），以及检测和响应集群事件（例如，当不满足部署的 replicas 字段时，启动新的 pod）。 控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机（一般使用 master 节点）上启动所有控制平面组件，并且不会在此计算机上运行用户容器。 kube-apiserver API 服务器是 Kubernetes 控制面的组件， 该组件公开了 Kubernetes API。 API 服务器是 Kubernetes 控制面的前端。 Kubernetes API 服务器的主要实现是 kube-apiserver。 kube-apiserver 设计上考虑了水平伸缩，也就是说，它可通过部署多个实例进行伸缩。 你可以运行 kube-apiserver 的多个实例，并在这些实例之间平衡流量。 etcd etcd 是兼具一致性和高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。 你的 Kubernetes 集群的 etcd 数据库通常需要有个备份计划。 要了解 etcd 更深层次的信息，请参考 etcd 文档。 kube-scheduler 控制平面组件，负责监视新创建的、未指定运行节点（node）的 Pods，选择节点让 Pod 在上面运行。 调度决策考虑的因素包括单个 Pod 和 Pod 集合的资源需求、硬件/软件/策略约束、亲和性和反亲和性规范、数据位置、工作负载间的干扰和最后时限。 kube-controller-manager 在主节点上运行控制器的组件。 从逻辑上讲，每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在一个进程中运行。 这些控制器包括： 节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应。 副本控制器（Replication Controller）：负责为系统中的每个副本控制器对象维护正确数量的 Pod。 端点控制器（Endpoints Controller）：填充端点(Endpoints)对象(即加入 Service 与 Pod)。 服务帐户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌。 Node 组件 节点组件在每个节点上运行，维护运行的 Pod 并提供 Kubernetes 运行环境。 kubelet 一个在集群中每个节点（node）上运行的代理。 它保证容器（containers）都运行在 Pod 中。 kubelet 接收一组通过各类机制提供给它的 PodSpecs，确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。 kube-proxy kube-proxy 是集群中每个节点上运行的网络代理， 实现 Kubernetes 服务（Service） 概念的一部分。 kube-proxy 维护节点上的网络规则。这些网络规则允许从集群内部或外部的网络会话与 Pod 进行网络通信。 如果操作系统提供了数据包过滤层并可用的话，kube-proxy 会通过它来实现网络规则。否则， kube-proxy 仅转发流量本身。 容器运行时（Container Runtime） 容器运行环境是负责运行容器的软件。 Kubernetes 支持多个容器运行环境：Docker、 containerd、CRI-O 以及任何实现 Kubernetes CRI (容器运行环境接口)。 插件（Addons） 插件使用 Kubernetes 资源（DaemonSet、 Deployment等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。 DNS 尽管其他插件都并非严格意义上的必需组件，但几乎所有 Kubernetes 集群都应该有集群 DNS， 因为很多示例都需要 DNS 服务。 集群 DNS 是一个 DNS 服务器，和环境中的其他 DNS 服务器一起工作，它为 Kubernetes 服务提供 DNS 记录。 Kubernetes 启动的容器自动将此 DNS 服务器包含在其 DNS 搜索列表中。 Web 界面（仪表盘） Dashboard 是Kubernetes 集群的通用、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身并进行故障排除。 Ingress Controller 为了让 Ingress 资源工作，集群必须有一个正在运行的 Ingress 控制器。 容器资源监控 容器资源监控将关于容器的一些常见的时间序列度量值保存到一个集中的数据库中，并提供用于浏览这些数据的界面，参考 Prometheus 项目 结合 Grafana 项目。 集群层面日志 集群层面日志 机制负责将容器的日志数据保存到一个集中的日志存储中，该存储能够提供搜索和浏览接口。 Kubernetes API Kubernetes 控制面的核心是 API 服务器。 API 服务器负责提供 HTTP API，以供用户、集群中的不同部分和集群外部组件相互通信。 Kubernetes API 使你可以查询和操纵 Kubernetes API 中对象（例如：Pod、Namespace、ConfigMap 和 Event）的状态。 大部分操作都可以通过 kubectl 命令行接口或类似 kubeadm 这类命令行工具来执行， 这些工具在背后也是调用 API。不过，你也可以使用 REST 调用来访问这些 API。 使用 Kubernetes 对象 理解 Kubernetes 对象 本页说明了 Kubernetes 对象在 Kubernetes API 中是如何表示的，以及如何在 .yaml 格式的文件中表示。 理解 Kubernetes 对象 在 Kubernetes 系统中，Kubernetes 对象是持久化的实体。 Kubernetes 使用这些实体去表示整个集群的状态。特别地，它们描述了如下信息： 哪些容器化应用在运行（以及在哪些节点上运行）。 可以被应用使用的资源。 关于应用运行时表现的策略，比如重启策略、升级策略，以及容错策略。 Kubernetes 对象是 “目标性记录” —— 一旦创建对象，Kubernetes 系统将持续工作以确保对象存在。 通过创建对象，本质上是在告知 Kubernetes 系统，所需要的集群工作负载看起来是什么样子的， 这就是 Kubernetes 集群的期望状态（Desired State）。 操作 Kubernetes 对象 —— 无论是创建、修改，或者删除 —— 都需要使用 Kubernetes API。 比如，当使用 kubectl 命令行接口时，CLI 会执行必要的 Kubernetes API 调用， 也可以在程序中使用 客户端库直接调用 Kubernetes API。 对象规约（Spec）与状态（Status） 几乎每个 Kubernetes 对象包含两个嵌套的对象字段，它们负责管理对象的配置： 对象 spec（规约）和对象 status（状态） 。 对于具有 spec 的对象，你必须在创建对象时设置其内容，描述你希望对象所具有的特征： 期望状态（Desired State） 。 status 描述了对象的当前状态（Current State），它是由 Kubernetes 系统和组件设置并更新的。在任何时刻，Kubernetes 控制平面 都一直积极地管理着对象的实际状态，以使之与期望状态相匹配。 描述 Kubernetes 对象 创建 Kubernetes 对象时，必须提供对象的规约，用来描述该对象的期望状态， 以及关于对象的一些基本信息（例如名称）。 当使用 Kubernetes API 创建对象时（或者直接创建，或者基于kubectl）， API 请求必须在请求体中包含 JSON 格式的信息。 大多数情况下，需要在 .yaml 文件中为 kubectl 提供这些信息。kubectl 在发起 API 请求时，会将这些信息转换成 JSON 格式。 必需字段 在想要创建的 Kubernetes 对象对应的 .yaml 文件中，需要配置如下的字段： apiVersion - 创建该对象所使用的 Kubernetes API 的版本。 kind - 想要创建的对象的类别。 metadata - 帮助唯一性标识对象的一些数据，包括一个 name 字符串、UID 和可选的 namespace。 Kubernetes 对象管理 命令式命令 用命令式命令时，用户可以在集群中的活动对象上进行操作。用户将操作传给 kubectl 命令作为参数或标志。 这是开始或者在集群中运行一次性任务的最简单方法。因为这个技术直接在活动对象上操作，所以它不提供以前配置的历史记录。 通过创建 Deployment 对象来运行 nginx 容器的实例： kubectl run nginx --image nginx 命令式对象配置 在命令式对象配置中，kubectl 命令指定操作（创建，替换等），可选标志和至少一个文件名。指定的文件必须包含 YAML 或 JSON 格式的对象的完整定义。 创建配置文件中定义的对象： kubectl create -f nginx.yaml 对象名称和 IDs 集群中的每一个对象都有一个名称来标识在同类资源中的唯一性。 每个 Kubernetes 对象也有一个 UID 来标识在整个集群中的唯一性。 比如，在同一个名字空间中有一个名为 myapp-1234 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 myapp-1234。 对于用户提供的非唯一性的属性，Kubernetes 提供了 标签（Labels）和 注解（Annotation）机制。 名字空间 Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。 这些虚拟集群被称为名字空间。 何时使用多个名字空间 名字空间适用于存在很多跨多个团队或项目的用户的场景。对于只有几到几十个用户的集群，根本不需要创建或考虑名字空间。当需要名称空间提供的功能时，请开始使用它们。 名字空间为名称提供了一个范围。资源的名称需要在名字空间内是唯一的，但不能跨名字空间。 名字空间不能相互嵌套，每个 Kubernetes 资源只能在一个名字空间中。 名字空间是在多个用户之间划分集群资源的一种方法（通过资源配额）。 不需要使用多个名字空间来分隔轻微不同的资源，例如同一软件的不同版本： 使用标签来区分同一名字空间中的不同资源。 使用名字空间 名字空间的创建和删除在名字空间的管理指南文档描述。 说明： 避免使用前缀 kube- 创建名字空间，因为它是为 Kubernetes 系统名字空间保留的。 查看名字空间 你可以使用以下命令列出集群中现存的名字空间： kubectl get namespace Kubernetes 会创建四个初始名字空间： default 没有指明使用其它名字空间的对象所使用的默认名字空间。 kube-system Kubernetes 系统创建对象所使用的名字空间。 kube-public 这个名字空间是自动创建的，所有用户（包括未经过身份验证的用户）都可以读取它。 这个名字空间主要用于集群使用，以防某些资源在整个集群中应该是可见和可读的。 这个名字空间的公共方面只是一种约定，而不是要求。 kube-node-lease 此名字空间用于与各个节点相关的租期（Lease）对象； 此对象的设计使得集群规模很大时节点心跳检测性能得到提升。 为请求设置名字空间 要为当前请求设置名字空间，请使用 --namespace 参数。 例如： kubectl run nginx --image=nginx --namespace= kubectl get pods --namespace= 设置名字空间偏好 你可以永久保存名字空间，以用于对应上下文中所有后续 kubectl 命令。 名字空间和 DNS 当你创建一个服务 时， Kubernetes 会创建一个相应的 DNS 条目。 该条目的形式是 ..svc.cluster.local，这意味着如果容器只使用 ，它将被解析到本地名字空间的服务。这对于跨多个名字空间（如开发、分级和生产） 使用相同的配置非常有用。如果你希望跨名字空间访问，则需要使用完全限定域名（FQDN）。 并非所有对象都在名字空间中 大多数 kubernetes 资源（例如 Pod、Service、副本控制器等）都位于某些名字空间中。 但是名字空间资源本身并不在名字空间中。而且底层资源，例如 节点 和持久化卷不属于任何名字空间。 查看哪些 Kubernetes 资源在名字空间中，哪些不在名字空间中： # 位于名字空间中的资源 kubectl api-resources --namespaced=true # 不在名字空间中的资源 kubectl api-resources --namespaced=false 标签和选择算符 标签（Labels）是附加到 Kubernetes 对象（比如 Pods）上的键值对。 标签旨在用于指定对用户有意义且相关的对象的标识属性，但不直接对核心系统有语义含义。 标签可以用于组织和选择对象的子集。标签可以在创建时附加到对象，随后可以随时添加和修改。 每个对象都可以定义一组键/值标签。每个键对于给定对象必须是唯一的。 动机 标签使用户能够以松散耦合的方式将他们自己的组织结构映射到系统对象，而无需客户端存储这些映射。 服务部署和批处理流水线通常是多维实体（例如，多个分区或部署、多个发行序列、多个层，每层多个微服务）。 管理通常需要交叉操作，这打破了严格的层次表示的封装，特别是由基础设施而不是用户确定的严格的层次结构。 示例标签： \"release\" : \"stable\", \"release\" : \"canary\" \"environment\" : \"dev\", \"environment\" : \"qa\", \"environment\" : \"production\" \"tier\" : \"frontend\", \"tier\" : \"backend\", \"tier\" : \"cache\" \"partition\" : \"customerA\", \"partition\" : \"customerB\" \"track\" : \"daily\", \"track\" : \"weekly\" 这些只是常用标签的例子; 你可以任意制定自己的约定。请记住，对于给定对象标签的键必须是唯一的。 语法和字符集 标签是键值对。有效的标签键有两个段：可选的前缀和名称，用斜杠（/）分隔。 名称段是必需的，必须小于等于 63 个字符，以字母数字字符（[a-z0-9A-Z]）开头和结尾， 带有破折号（-），下划线（_），点（ .）和之间的字母数字。 前缀是可选的。如果指定，前缀必须是 DNS 子域：由点（.）分隔的一系列 DNS 标签，总共不超过 253 个字符， 后跟斜杠（/）。 如果省略前缀，则假定标签键对用户是私有的。 向最终用户对象添加标签的自动系统组件（例如 kube-scheduler、kube-controller-manager、 kube-apiserver、kubectl 或其他第三方自动化工具）必须指定前缀。 kubernetes.io/ 前缀是为 Kubernetes 核心组件保留的。 有效标签值必须为 63 个字符或更少，并且必须为空或以字母数字字符（[a-z0-9A-Z]）开头和结尾， 中间可以包含破折号（-）、下划线（_）、点（.）和字母或数字。 标签选择算符 与名称和 UID 不同， 标签不支持唯一性。通常，我们希望许多对象携带相同的标签。 通过 标签选择算符，客户端/用户可以识别一组对象。标签选择算符是 Kubernetes 中的核心分组原语。 API 目前支持两种类型的选择算符：基于等值的 和 基于集合的。标签选择算符可以由逗号分隔的多个 需求 组成。 在多个需求的情况下，必须满足所有要求，因此逗号分隔符充当逻辑 与（&&）运算符。 空标签选择算符或者未指定的选择算符的语义取决于上下文， 支持使用选择算符的 API 类别应该将算符的合法性和含义用文档记录下来。 基于等值的需求 基于等值 或 基于不等值 的需求允许按标签键和值进行过滤。 匹配对象必须满足所有指定的标签约束，尽管它们也可能具有其他标签。 可接受的运算符有 = 、 == 和 != 三种。 前两个表示相等（并且只是同义词），而后者表示不相等。例如： environment = production tier != frontend 基于集合的需求 基于集合 的标签需求允许你通过一组值来过滤键。 支持三种操作符：in、notin 和 exists (只可以用在键标识符上)。例如： environment in (production, qa) tier notin (frontend, backend) partition !partition 第一个示例选择了所有键等于 environment 并且值等于 production 或者 qa 的资源。 第二个示例选择了所有键等于 tier 并且值不等于 frontend 或者 backend 的资源，以及所有没有 tier 键标签的资源。 第三个示例选择了所有包含了有 partition 标签的资源；没有校验它的值。 第四个示例选择了所有没有 partition 标签的资源；没有校验它的值。 类似地，逗号分隔符充当 与 运算符。因此，使用 partition 键（无论为何值）和 environment 不同于 qa 来过滤资源可以使用 partition, environment notin（qa) 来实现。 基于集合的标签选择算符是相等标签选择算符的一般形式，因为 environment=production 等同于 environment in（production）；!= 和 notin 也是类似的。 基于集合的要求可以与基于相等的要求混合使用。例如：partition in (customerA, customerB),environment!=qa。 注解 你可以使用 Kubernetes 注解为对象附加任意的非标识的元数据。客户端程序（例如工具和库）能够获取这些元数据信息。 为对象附加元数据 你可以使用标签或注解将元数据附加到 Kubernetes 对象。 标签可以用来选择对象和查找满足某些条件的对象集合。 相反，注解不用于标识和选择对象。 注解中的元数据，可以很小，也可以很大，可以是结构化的，也可以是非结构化的，能够包含标签不允许的字符。 注解和标签一样，是键/值对： \"metadata\": { \"annotations\": { \"key1\" : \"value1\", \"key2\" : \"value2\" } } 语法和字符集 注解（Annotations）存储的形式是键/值对。有效的注解键分为两部分： 可选的前缀和名称，以斜杠（/）分隔。 名称段是必需项，并且必须在63个字符以内，以字母数字字符（[a-z0-9A-Z]）开头和结尾， 并允许使用破折号（-），下划线（_），点（.）和字母数字。 前缀是可选的。如果指定，则前缀必须是DNS子域：一系列由点（.）分隔的DNS标签， 总计不超过253个字符，后跟斜杠（/）。 如果省略前缀，则假定注解键对用户是私有的。 由系统组件添加的注解 （例如，kube-scheduler，kube-controller-manager，kube-apiserver，kubectl 或其他第三方组件），必须为终端用户添加注解前缀。 kubernetes.io/ 和 k8s.io/ 前缀是为 Kubernetes 核心组件保留的。 例如，下面是一个 Pod 的配置文件，其注解中包含 imageregistry: apiVersion: v1 kind: Pod metadata: name: annotations-demo annotations: imageregistry: \"https://hub.docker.com/\" spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 字段选择器 字段选择器（Field selectors）允许你根据一个或多个资源字段的值 筛选 Kubernetes 资源。 下面是一些使用字段选择器查询的例子： metadata.name=my-service metadata.namespace!=default status.phase=Pending 下面这个 kubectl 命令将筛选出 status.phase 字段值为 Running 的所有 Pod： kubectl get pods --field-selector status.phase=Running 支持的字段 你可在字段选择器中使用 =、==和 != （= 和 == 的意义是相同的）操作符。 例如，下面这个 kubectl 命令将筛选所有不属于 default 命名空间的 Kubernetes 服务： kubectl get services --all-namespaces --field-selector metadata.namespace!=default 链式选择器 同标签和其他选择器一样， 字段选择器可以通过使用逗号分隔的列表组成一个选择链。 下面这个 kubectl 命令将筛选 status.phase 字段不等于 Running 同时 spec.restartPolicy 字段等于 Always 的所有 Pod： kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always 多种资源类型 你能够跨多种资源类型来使用字段选择器。 下面这个 kubectl 命令将筛选出所有不在 default 命名空间中的 StatefulSet 和 Service： kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default Kubernetes 架构 节点 Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。 节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。 每个节点包含运行 Pods 所需的服务， 这些 Pods 由控制面负责管理。 通常集群中会有若干个节点；而在一个学习用或者资源受限的环境中，你的集群中也可能只有一个节点。 节点上的组件包括 kubelet、 容器运行时以及 kube-proxy。 管理 向 API 服务器添加节点的方式主要有两种： 节点上的 kubelet 向控制面执行自注册。 你，或者别的什么人，手动添加一个 Node 对象。 在你创建了 Node 对象或者节点上的 kubelet 执行了自注册操作之后， 控制面会检查新的 Node 对象是否合法。例如，如果你使用下面的 JSON 对象来创建 Node 对象： { \"kind\": \"Node\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"10.240.79.157\", \"labels\": { \"name\": \"my-first-k8s-node\" } } } Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 kubelet 向 API 服务器注册节点时使用的 metadata.name 字段是否匹配。 如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。 否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。 控制面到节点通信 本文列举控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。 目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，使得集群能够在不可信的网络上（或者在一个云服务商完全公开的 IP 上）运行。 节点到控制面 Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。 所有从集群（或所运行的 Pods）发出的 API 调用都终止于 apiserver（其它控制面组件都没有被设计为可暴露远程服务）。 apiserver 被配置为在一个安全的 HTTPS 端口（443）上监听远程连接请求， 并启用一种或多种形式的客户端身份认证机制。 一种或多种客户端鉴权机制应该被启用， 特别是在允许使用匿名请求 或服务账号令牌的时候。 应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 apiserver。 一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。 请查看 kubelet TLS 启动引导 以了解如何自动提供 kubelet 客户端证书。 想要连接到 apiserver 的 Pod 可以使用服务账号安全地进行连接。 当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。 kubernetes 服务（位于所有名字空间中）配置了一个虚拟 IP 地址，用于（通过 kube-proxy）转发 请求到 apiserver 的 HTTPS 末端。 控制面组件也通过安全端口与集群的 apiserver 通信。 这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的， 能够在不可信的网络或公网上运行。 控制面到节点 从控制面（apiserver）到节点有两种主要的通信路径。 第一种是从 apiserver 到集群中每个节点上运行的 kubelet 进程。 第二种是从 apiserver 通过它的代理功能连接到任何节点、Pod 或者服务。 API 服务器到 kubelet 从 apiserver 到 kubelet 的连接用于： 获取 Pod 日志。 挂接（通过 kubectl）到运行中的 Pod。 提供 kubelet 的端口转发功能。 这些连接终止于 kubelet 的 HTTPS 末端。 默认情况下，apiserver 不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击， 在非受信网络或公开网络上运行也是 不安全的。 为了对这个连接进行认证，使用 --kubelet-certificate-authority 标志给 apiserver 提供一个根证书包，用于 kubelet 的服务证书。 如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 apiserver 和 kubelet 之间使用 SSH 隧道。 最后，应该启用 kubelet 用户认证和/或鉴权 来保护 kubelet API。 apiserver 到节点、Pod 和服务 从 apiserver 到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。 这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 https: 来运行在安全的 HTTPS 连接上。 不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。 因此，虽然连接是加密的，仍无法提供任何完整性保证。 这些连接目前还不能安全地在非受信网络或公共网络上运行。 工作负载 工作负载是在 Kubernetes 上运行的应用程序。 无论你的负载是单一组件还是由多个一同工作的组件构成，在 Kubernetes 中你可以在一组 Pods 中运行它。 在 Kubernetes 中，Pod 代表的是集群上处于运行状态的一组容器。 Kubernetes Pods 有确定的生命周期。 例如，一旦某 Pod 在你的集群中运行，Pod 运行所在的节点出现致命错误时， 所有该节点上的 Pods 都会失败。Kubernetes 将这类失败视为最终状态：即使该节点后来恢复正常运行，你也需要创建新的 Pod 来恢复应用。 不过，为了让用户的日子略微好过一些，你并不需要直接管理每个 Pod。 相反，你可以使用负载资源来替你管理一组 Pods。 这些资源配置控制器来确保合适类型的、处于运行状态的 Pod 个数是正确的，与你所指定的状态相一致。 Kubernetes 提供若干种内置的工作负载资源： Deployment 和 ReplicaSet （替换原来的资源 ReplicationController）。 Deployment 很适合用来管理你的集群上的无状态应用，Deployment 中的所有 Pod 都是相互等价的，并且在需要的时候被换掉。 StatefulSet 让你能够运行一个或者多个以某种方式跟踪应用状态的 Pods。 例如，如果你的负载会将数据作持久存储，你可以运行一个 StatefulSet，将每个 Pod 与某个 PersistentVolume 对应起来。你在 StatefulSet 中各个 Pod 内运行的代码可以将数据复制到同一 StatefulSet 中的其它 Pod 中以提高整体的服务可靠性。 DaemonSet 定义提供节点本地支撑设施的 Pods。这些 Pods 可能对于你的集群的运维是非常重要的，例如作为网络链接的辅助工具或者作为网络插件的一部分等等。每次你向集群中添加一个新节点时，如果该节点与某 DaemonSet 的规约匹配，则控制面会为该 DaemonSet 调度一个 Pod 到该新节点上运行。 Job 和 CronJob。 定义一些一直运行到结束并停止的任务。Job 用来表达的是一次性的任务，而 CronJob 会根据其时间规划反复运行。 pods Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。 Pod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个） 容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。 Pod 所建模的是特定于应用的“逻辑主机”，其中包含一个或多个应用容器， 这些容器是相对紧密的耦合在一起的。 在非云环境中，在相同的物理机或虚拟机上运行的应用类似于在同一逻辑主机上运行的云应用。 除了应用容器，Pod 还可以包含在 Pod 启动期间运行的 Init 容器。 你也可以在集群中支持临时性容器 的情况外，为调试的目的注入临时性容器。 什么是 Pod？ 说明： 除了 Docker 之外，Kubernetes 支持很多其他容器运行时（容器运行时是负责运行容器的软件）， Docker 是最有名的运行时， 使用 Docker 的术语来描述 Pod 会很有帮助。 Pod 的共享上下文包括一组 Linux 名字空间、控制组（cgroup）和可能一些其他的隔离方面，即用来隔离 Docker 容器的技术。 在 Pod 的上下文中，每个独立的应用可能会进一步实施隔离。 就 Docker 概念的术语而言，Pod 类似于共享名字空间和文件系统卷的一组 Docker 容器。 使用 Pod 通常你不需要直接创建 Pod，甚至单实例 Pod。 相反，你会使用诸如 Deployment 或 Job 这类工作负载资源来创建 Pod。如果 Pod 需要跟踪状态， 可以考虑 StatefulSet 资源。 Kubernetes 集群中的 Pod 主要有两种用法： 运行单个容器的 Pod。“每个 Pod 一个容器” 模型是最常见的 Kubernetes 用例； 在这种情况下，可以将 Pod 看作单个容器的包装器，并且 Kubernetes 直接管理 Pod，而不是容器。 运行多个协同工作的容器的 Pod。 Pod 可能封装由多个紧密耦合且需要共享资源的共处容器组成的应用程序。 这些位于同一位置的容器可能形成单个内聚的服务单元 —— 一个容器将文件从共享卷提供给公众， 而另一个单独的 “挂斗”（sidecar）容器则刷新或更新这些文件。 Pod 将这些容器和存储资源打包为一个可管理的实体。 说明： 将多个并置、同管的容器组织到一个 Pod 中是一种相对高级的使用场景。 只有在一些场景中，容器之间紧密关联时你才应该使用这种模式。 每个 Pod 都旨在运行给定应用程序的单个实例。如果希望横向扩展应用程序（例如，运行多个实例以提供更多的资源），则应该使用多个 Pod，每个实例使用一个 Pod。 在 Kubernetes 中，这通常被称为副本（Replication）。 通常使用一种工作负载资源及其控制器来创建和管理一组 Pod 副本。 工作负载资源 工作负载是指在 Kubernetes 上运行的应用程序，一个 pod 可以运行一个或多个工作负载。工作负载资源是用来管理一组 pod 的，确保这些被管理的 pod ，处于我们期望的运行状态。 Deployments 一个 Deployment 为 Pods （pod 表示集群上正在运行的一组容器） 和 ReplicaSets （下一代副本控制器）提供声明式的更新能力。 你负责描述 Deployment 中的目标状态，而 Deployment 控制器（Controller）以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment， 并通过新的 Deployment 收养其资源。 ReplicaSet ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。 StatefulSets StatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 DaemonSet DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 DaemonSet 的一些典型用法： 在每个节点上运行集群守护进程。 在每个节点上运行日志收集守护进程。 在每个节点上运行监控守护进程。 一种简单的用法是为每种类型的守护进程在所有的节点上都启动一个 DaemonSet。 一个稍微复杂的用法是为同一种守护进程部署多个 DaemonSet；每个具有不同的标志， 并且对不同硬件类型具有不同的内存、CPU 要求。 Jobs Job 会创建一个或者多个 Pods，并将继续重试 Pods 的执行，直到指定数量的 Pods 成功终止。 随着 Pods 成功结束，Job 跟踪记录成功完成的 Pods 个数。 当数量达到指定的成功个数阈值时，任务（即 Job）结束。 删除 Job 的操作会清除所创建的全部 Pods。 一种简单的使用场景下，你会创建一个 Job 对象以便以一种可靠的方式运行某 Pod 直到完成。 当第一个 Pod 失败或者被删除（比如因为节点硬件失效或者重启）时，Job 对象会启动一个新的 Pod。 CronJob CronJob 创建基于时间调度的 Jobs。 一个 CronJob 对象就像 crontab (cron table) 文件中的一行。 它用 Cron 格式进行编写， 并周期性地在给定的调度时间执行 Job。 ReplicationController ReplicationController 确保在任何时候都有特定数量的 Pod 副本处于运行状态。 换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。 ReplicationController 的替代方案 ReplicaSet ReplicaSet 是下一代 ReplicationController， 支持新的基于集合的标签选择算符。 它主要被 Deployment 用来作为一种编排 Pod 创建、删除及更新的机制。 请注意，我们推荐使用 Deployment 而不是直接使用 ReplicaSet，除非 你需要自定义更新编排或根本不需要更新。 Deployment （推荐） Deployment 是一种更高级别的 API 对象， 它以类似于 kubectl rolling-update 的方式更新其底层 ReplicaSet 及其 Pod。 如果你想要这种滚动更新功能，那么推荐使用 Deployment，因为与 kubectl rolling-update 不同， 它们是声明式的、服务端的，并且具有其它特性。 "},"worknote/kubernetes/deployment-kubernetes-cluster.html":{"url":"worknote/kubernetes/deployment-kubernetes-cluster.html","title":"部署Kubernetes集群","keywords":"","body":"部署 Kubernetes 集群 Kubernetes 官方提供多种部署方式： 云解决方案。 使用部署工具安装 Kubernetes。 Windows Kubernetes。 我们这里使用部署工具来安装 Kubernetes，官方提供的工具主要有以下几个： Kubeadm: 官方维护的为了给创建 Kubernetes 集群提供最佳实践的一个工具，涉及集群生命周期管理等知识。 Kops: 在 AWS 上安装 Kubernetes 集群。 Kubespray: Ansible 部署，OS 级别通用的部署方式，可以是裸机和云的环境。 这里我们使用 Kubeadm 部署，这是官方推荐的部署方式，Kubeadm 可用于生产级别的集群部署。 先决条件 一台或多台运行着下列系统的机器： Ubuntu 16.04+ Debian 9+ CentOS 7+ Red Hat Enterprise Linux (RHEL) 7+ Fedora 25+ HypriotOS v1.0.1+ Flatcar Container Linux （使用 2512.3.0 版本测试通过） 每台机器 2 GB 或更多的 RAM （如果少于这个数字将会影响你应用的运行内存)。 2 CPU 核或更多。 集群中的所有机器的网络彼此均能相互连接(公网和内网都可以)。 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。 开启机器上的某些端口（Kubernetes 服务所占用的端口必须开启）。 禁用交换分区。为了保证 kubelet 正常工作，你必须禁用交换分区。 环境配置 节点准备 k8s-master 10.10.110.190 k8s-ndoe1 10.10.110.191 k8s-node2 10.10.110.192 Operating System: Ubuntu 18.04.5 LTS 架构图 配置节点 # 关闭防火墙 systemctl stop ufw ufw disable # 检查所有节点网络接口的mac地址和product_uuid唯一性 ip link cat /sys/class/dmi/id/product_uuid # 禁用swap分区 swapoff -a # 设置主机名 hostnamectl set-hostname [hostname] # 配置hosts解析 cat >> /etc/hosts /dev/null\" | crontab 安装 Container Runtime 为了在 Pod 中运行容器，Kubernetes 需要使用容器运行时。 # 卸载旧版本 apt-get remove docker docker-engine docker.io containerd runc # 更新apt包索引和安装包 apt-get update apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release # 添加Docker的官方GPG密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # 设置稳定存储库 echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null # 安装Docker引擎 apt-get update apt-get install -y docker-ce docker-ce-cli containerd.io # 配置docker镜像加速,kubernetes官方建议docker驱动采用systemd,如果不修改kubeadm init时会有warning提示 cat 安装 kubeadm、kubelet 和 kubectl 你需要在每台机器上安装以下的软件包： kubeadm：用来初始化集群的指令。 kubelet：在集群中的每个节点上用来启动 Pod 和容器等。 kubectl：用来与集群通信的命令行工具。 kubeadm 不能帮你安装或者管理 kubelet 或 kubectl，所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。 apt-get update && apt-get install -y apt-transport-https curl curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat 初始化控制平面节点 控制平面节点是运行控制平面组件的机器， 包括 etcd （集群数据库） 和 API Server（命令行工具 kubectl 与之通信）。 在所有 master 节点执行 kubeadm init \\ --apiserver-advertise-address=10.10.110.190 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.21.0 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 \\ --ignore-preflight-errors=all --apiserver-advertise-address 服务器所公布的其正在监听的 IP 地址。 --image-repository 默认拉取镜像地址为 k8s.gcr.io（国内网络无法拉取），这里指定阿里云镜像仓库地址。 --kubernetes-version 指定 k8s 安装版本。 --service-cidr 集群内部虚拟网络，pod 统一访问入口。 --pod-network-cidr 指明 pod 网络可以使用的 IP 地址段。 拷贝 kubectl 连接 k8s 所使用的认证文件到当前用户的默认路径。 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装 Pod 网络附加组件 你必须部署一个基于 Pod 网络插件的容器网络接口 (CNI)，以便你的 Pod 可以相互通信。 在安装网络之前，集群 DNS (CoreDNS) 将不会启动。这也是为什么 node 的状态其实是 NotReady 的原因。Kubernetes 常用的网络插件包括 Calico、Flannel、Canal 和 Weave，这里我们使用 Calico 来为 Kubernetes 集群提供网络策略支持。 你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件： # 下载 calico 官方配置文件(国内网络可能会下载失败) wget https://docs.projectcalico.org/manifests/calico.yaml # 修改 calico 配置文件 - name: CALICO_IPV4POOL_CIDR value: \"10.244.0.0/16\" # 修改为kubeadm init时指定的--pod-network-cidr网段 # 应用配置文件 kubectl apply -f calico.yaml # 查看 pods 运行状态 kubectl get pods -n kube-system 加入节点 节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作： SSH 到机器。 成为 root （例如 sudo su -）。 运行 kubeadm init 输出的命令。例如： kubeadm join 10.10.110.190:6443 --token 54sx6k.gi533yr3f4yimvky \\ --discovery-token-ca-cert-hash sha256:3f16fb0f5c1ed611af164b8f5df6891ee60bba760286b860d125d08a304ed4b0 执行 kubeadm init 之后，默认生成的 token 有效期为 24 小时，过期之后就需要重新创建 token，操作如下： # 列出token列表 kubeadm token list # 创建token kubeadm token create bvw33z.dd7p7h2t151vc6ej # 这里是新生成的token # 获取CA证书公钥哈希值 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' 3f16fb0f5c1ed611af164b8f5df6891ee60bba760286b860d125d08a304ed4b0 # 使用新的token和公钥哈希值加入节点 kubeadm join 10.10.110.190:6443 --token $(新生成的token) \\ --discovery-token-ca-cert-hash sha256:$(新生成的公钥哈希值) 部署 Dashboard Dashboard 是基于网页的 Kubernetes 用户界面。 你可以使用 Dashboard 将容器应用部署到 Kubernetes 集群中，也可以对容器应用排错，还能管理集群资源。 你可以使用 Dashboard 获取运行在集群中的应用的概览信息，也可以创建或者修改 Kubernetes 资源 （如 Deployment，Job，DaemonSet 等等）。 例如，你可以对 Deployment 实现弹性伸缩、发起滚动升级、重启 Pod 或者使用向导创建新的应用。 默认情况下不会部署 Dashboard。可以通过以下命令部署： # 下载dashboard配置清单文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml # dashboard默认的service是ClusterIP类型,我们需要修改为NodePort类型,才能让外部访问到我们的dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 nodePort: 30023 selector: k8s-app: kubernetes-dashboard type: NodePort --- # 访问地址 https://nodeip:30023 访问 Dashboard 管理页面所需的 token，可以通过以下命令创建： # 我们首先在kubernetes-dashboard命名空间中创建名为admin-user的service account kubectl create serviceaccount admin-user -n kubernetes-dashboard kubectl get serviceaccounts -n kubernetes-dashboard # 创建集群角色绑定,给admin-user用户授权 kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:admin-user # 获取kubernetes-dashboard命名空间下admin-user用户的登录token kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 设置 k8s 命令自动补全 apt-get install -y bash-completion locate bash_completion source /usr/share/bash-completion/bash_completion source > ~/.bashrc "},"worknote/kubernetes/kubelet-overview.html":{"url":"worknote/kubernetes/kubelet-overview.html","title":"Kubelet概述","keywords":"","body":"Kubelet 概述 我们可以使用 Kubectl 命令行工具管理 Kubernetes 集群。默认情况下，kubectl 在 $HOME/.kube 目录下查找名为 config 的文件。 你可以通过设置 KUBECONFIG 环境变量或者设置 --kubeconfig参数来指定其他 kubeconfig 文件。使用 kubeconfig 文件来组织有关集群、用户、命名空间和身份认证机制的信息。kubectl 命令行工具使用 kubeconfig 文件来查找选择集群所需的信息，并与集群的 API 服务器进行通信。 说明：用于配置集群访问的文件称为 kubeconfig 文件。这是引用配置文件的通用方法。这并不意味着有一个名为 kubeconfig 的文件。 支持多集群、用户和身份认证机制 假设您有多个集群，并且您的用户和组件以多种方式进行身份认证。比如： 正在运行的 kubelet 可能使用证书在进行认证。 用户可能通过令牌进行认证。 管理员可能拥有多个证书集合提供给各用户。 使用 kubeconfig 文件，您可以组织集群、用户和命名空间。您还可以定义上下文，以便在集群和命名空间之间快速轻松地切换。 上下文（Context） 通过 kubeconfig 文件中的 context 元素，使用简便的名称来对访问参数进行分组。每个上下文都有三个参数：cluster、namespace 和 user。默认情况下，kubectl 命令行工具使用当前上下文中的参数与集群进行通信。 选择当前上下文 kubectl config use-context KUBECONFIG 环境变量 KUBECONFIG 环境变量包含一个 kubeconfig 文件列表。 对于 Linux 和 Mac，列表以冒号分隔。对于 Windows，列表以分号分隔。 KUBECONFIG 环境变量不是必要的。 如果 KUBECONFIG 环境变量不存在，kubectl 使用默认的 kubeconfig 文件，$HOME/.kube/config。 如果 KUBECONFIG 环境变量存在，kubectl 使用 KUBECONFIG 环境变量中列举的文件合并后的有效配置。 语法 使用以下语法 kubectl 从终端窗口运行命令： kubectl [command] [TYPE] [NAME] [flags] command: 指定要对一个或多个资源执行的操作，例如 create、get、describe、delete。 TYPE: 指定资源类型。资源类型不区分大小写， 可以指定单数、复数或缩写形式。 NAME: 指定资源的名称。名称区分大小写。 如果省略名称，则显示所有资源的详细信息 kubectl get pods。 flags: 指定可选的参数。 Kubectl --help 基本指令(初级) create: 从文件或标准输入创建资源。 expose: 将资源作为新的 Kubernetes 服务公开。 run: 在集群上运行指定的镜像。 set: 设置对象的特定功能。 基本指令(中级) explain: 解释文档参考资料。 get: 显示一个或多个资源。 edit: 编辑服务器上的资源。 delete: 通过文件名、标准输入、资源和名称或通过资源和标签选择器删除资源。 部署命令 rollout: 管理资源的发布。 scale: 对 deployment、ReplicaSet、Replication Controller 或 StatefulSet 扩容缩容。 autoscale: 自动扩容缩容。 集群管理命令 certificate: 修改证书资源。 cluster-info: 显示集群信息。 top: 显示资源(CPU/内存/存储)使用情况。 cordon: 将节点标记为不可调度。 uncordon: 将节点标记为可调度的。 drain: 更新一个或多个节点上的污点。 taint: 驱逐节点上的应用准备下线维护。 疑难解答和调试命令 describe: 描述显示特定资源或资源组的详细信息。 logs: 打印一个容器的日志。 attach: 连接到正在运行的容器上。 exec: 在容器中执行命令。 port-forward: 转发一个或多个本地端口到一个 pod。 proxy: 运行到 Kubernetes API 服务器的代理。 cp: 在容器和容器之间复制文件和目录。 auth: 授权检查。 debug: 使用交互式调试容器调试群集资源。 高级命令 diff: 由文件名或标准输入指定当前联机配置和应用时的配置差异。 apply: 通过文件名或标准输入对资源应用配置。 patch: 使用补丁更新资源的字段。 replace: 用文件名或标准输入替换资源。 wait: 等待一个或多个资源的特定条件。 kustomize: 从 kustomization.yaml 文件中的指令生成一组 API 资源。 设置命令 label: 更新资源上的标签。 annotate: 更新一个或多个资源上的注解。 completion: kubectl 命令自动补全。 其他命令 api-resources: 打印受支持的 API 资源。 api-versions: 打印受支持的 API 版本。 config: 修改 Kubeconfig 文件。 plugin: 提供与插件交互的实用程序。 version: 打印当前上下文客户端和服务版本信息。 Kubectl 管理应用生命周期 创建应用 kubectl create deployment myapp-deployment --image=ikubernetes/myapp:v1 --replicas=3 发布应用 kubectl expose deployment myapp-deployment --name=myapp-service --type=NodePort --port=8000 --target-port=80 --protocol=TCP 升级应用 kubectl set image deployment/myapp-deployment myapp=ikubernetes/myapp:v2 kubectl rollout status deployment myapp-deployment # 查看升级状态 回滚应用 kubectl rollout history deployment myapp-deployment # 查看版本发布历史记录 kubectl rollout history deployment myapp-deployment --revision=3 # 查看指定版本发布的详细信息 kubectl rollout undo deployment myapp-deployment # 回滚到上一个版本 kubectl rollout undo deployment myapp-deployment --to-revision=2 # 回滚到指定的版本 删除应用 kubectl delete deployments.apps myapp-deployment kubectl delete service myapp-service "},"worknote/kubernetes/object-resource-format.html":{"url":"worknote/kubernetes/object-resource-format.html","title":"对象类资源格式","keywords":"","body":"对象类资源格式 Kubernetes API 仅支持接受及响应 JSON 格式的数据（JSON 对象），同时，为了便于使用，它也允许用户提供 YAML 格式的 POST 对象，但 API Server 接受和返回的所有 JSON 对象都遵循同一个模式，它们都具有 kind 和 apiVersion 字段，用于表示对象所属的资源类型、API 群组及相关版本。 大多数的对象或列表类型的资源还需要具有三个嵌套型的字段 metadata、spec、status。其中 metadata 字段为资源提供元数据信息，如名称、资源隶属的名称空间和标签等。spec 则用于定义用户期望的状态，不同的资源类型，其状态的意义也各不相同。status 则记录着活动对象的当前状态信息，它由 Kubernetes 系统自行维护，对用户来说为只读状态。 Kubectl 的命令可以分为三类：陈述式命令（Imperative commands）、陈述式对象配置（Imperative object configuration）和声明式对象配置（Declarative object configuration）。 陈述式命令就是此前管理应用生命周期用到的 run、expose 和 delete 等命令，它们直接作用于 Kubernetes 系统上的活动对象，简单易用，但是不支持代码复用、修改复审及审计日志的功能。对于新手来说，更容易上手学习。 陈述式对象配置管理方式支持使用 create、delete、get 和 replace 等命令，与陈述式命令不同之处在于，它通过资源配置清单读取需要管理的目标资源对象。陈述式对象配置管理操作同样直接作用于 Kubernetes 系统上的活动对象，即便修改配置清单中极小的一部分内容，使用 replace 命令进行的对象更新也会导致整个对象被替换。 声明式对象配置并不直接指明要进行的对象管理操作，而是提供配置清单文件给 Kubernetes 系统，并委托系统跟踪活动对象的状态变动。资源对象的创建、删除及修改操作可全部通过唯一的 apply 命令来完成。并且每次操作时，提供给命令的配置信息都存放于对象的注解信息中，并通过比对检查活动对象的当前状态、注解中的配置信息及资源清单中的配置信息三方进行变更合并，从而实现仅修改变动字段的高级补丁机制。 资源配置清单 我们前面使用 Kubectl 管理应用生命周期，使用的是陈述式命令。下面我们将以资源配置清单的格式来创建活动对象。 apiVersion: apps/v1 kind: Deployment metadata: name: ikubernetes-deployment labels: app: ikubernetes-deployment spec: replicas: 3 selector: matchLabels: app: ikubernetes template: metadata: labels: app: ikubernetes spec: containers: - name: ikubernetes image: ikubernetes/myapp:v1 ports: - containerPort: 80 在该活动对象实例中： 创建名为 ikubernetes-deployment （由 .metadata.name 字段标明）的 Deployment。 该 Deployment 创建三个（由 replicas 字段标明） Pod 副本。 selector 字段定义 Deployment 如何查找要管理的 Pods。这里只需选择在 Pod 模板中定义的标签（app: ikubernetes）。 template 字段包含以下子字段： Pod 被使用 labels 字段打上 app: ikubernetes 标签。 Pod 模板规约 （即 .template.spec 字段）指示 Pods 运行一个 ikubernetes 容器，并指定容器运行版本的镜像。 Kubernetes API 标准的资源组织格式由五个核心字段组成： apiVersion: 定义这个资源使用的 API 版本。 kind: 定义这个资源的类型。 metadata: 资源的元数据。 spec: 资源的规约，描述所期望的对象应有的状态。 status: 记录对象在系统上的当前状态。 在编写资源配置清单时如果对资源的字段不确定，可以使用 Kubernetes 内置的 explain 命令列出受支持资源的字段： # 获取资源及其字段的文档 kubectl explain pods # 获取资源的特定字段 kubectl explain pods.spec.containers "},"worknote/kubernetes/workload-resources.html":{"url":"worknote/kubernetes/workload-resources.html","title":"工作负载资源使用","keywords":"","body":"工作负载资源使用 Deployment 一个 Deployment 为 Pods 和 ReplicaSets 提供声明式的更新能力。 下面是 Deployment 示例。其中创建了一个 ReplicaSet，负责启动三个 nginx Pods: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 开始之前，请确保的 Kubernetes 集群已启动并运行。 按照以下步骤创建上述 Deployment: 通过运行以下命令创建 Deployment: kubectl apply -f nginx-deployment.yaml 运行 kubectl get deployments.apps -o wide 检查 Deployment 是否已创建： NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx-deployment 3/3 3 3 34s nginx nginx:1.14.2 app=nginx 在检查集群中的 Deployment 时，所显示的字段有： NAME 列出了集群中 Deployment 的名称。 READY 显示应用程序的可用的副本数，显示的模式是“就绪个数/期望个数”。 UP-TO-DATE 显示为了达到期望状态已经更新的副本数。 AVAILABLE 显示应用可供用户使用的副本数。 AGE 显示应用程序运行的时间。 请注意期望副本数是根据 .spec.replicas 字段设置 3。 运行 kubectl rollout status deployment nginx-deployment 查看 Deployment 上线状态： deployment \"nginx-deployment\" successfully rolled out 要查看 Deployment 创建的 ReplicaSet（rs），运行 kubectl get rs。 输出类似于： NAME DESIRED CURRENT READY AGE nginx-deployment-66b6c48dd5 3 3 3 5m5s 要查看每个资源自动生成的标签，运行 kubectl get pods --show-labels。返回以下输出： NAME READY STATUS RESTARTS AGE LABELS nginx-deployment-66b6c48dd5-5jd4w 1/1 Running 0 7m20s app=nginx,pod-template-hash=66b6c48dd5 nginx-deployment-66b6c48dd5-js2bx 1/1 Running 0 7m20s app=nginx,pod-template-hash=66b6c48dd5 nginx-deployment-66b6c48dd5-mqsnj 1/1 Running 0 7m20s app=nginx,pod-template-hash=66b6c48dd5 所创建的 ReplicaSet 确保总是存在三个 nginx Pod。 ReplicaSet ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。 因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。 ReplicaSet 确保任何时间都有指定数量的 Pod 副本在运行。 然而，Deployment 是一个更高级的概念，它管理 ReplicaSet，并向 Pod 提供声明式的更新以及许多其他有用的功能。 因此，我们建议使用 Deployment 而不是直接使用 ReplicaSet ，除非你需要自定义更新业务流程或根本不需要更新。 这实际上意味着，你可能永远不需要操作 ReplicaSet 对象：而是使用 Deployment，并在 spec 部分定义 ReplicaSet 管理你的应用。 apiVersion: apps/v1 kind: ReplicaSet metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: ti: fr template: metadata: labels: ti: fr spec: containers: - name: nginx image: nginx 你可以看到当前被部署的 ReplicaSet: kubectl get rs nginx 并看到你所创建的前端： NAME DESIRED CURRENT READY AGE nginx 3 3 2 11s 你也可以查看 ReplicaSet 的状态： kubectl describe rs nginx 你会看到类似如下的输出： Name: nginx Namespace: default Selector: ti=fr Labels: app=nginx Annotations: Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: ti=fr Containers: nginx: Image: nginx Port: Host Port: Environment: Mounts: Volumes: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 113s replicaset-controller Created pod: nginx-7hxhm Normal SuccessfulCreate 113s replicaset-controller Created pod: nginx-mnq9v Normal SuccessfulCreate 113s replicaset-controller Created pod: nginx-w9cfq 最后可以查看启动了的 Pods: kubectl get pods 你会看到类似如下的 Pod 信息： NAME READY STATUS RESTARTS AGE nginx-7hxhm 1/1 Running 0 3m2s nginx-mnq9v 1/1 Running 0 3m2s nginx-w9cfq 1/1 Running 0 3m2s StatefulSets DaemonSet DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 你可以在 YAML 文件中描述 DaemonSet。 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: quay.io/fluentd_elasticsearch/fluentd:v2.5.2 nodeSelector: type: ssd 如果指定了 .spec.template.spec.nodeSelector，DaemonSet 控制器将在能够与 Node 选择算符匹配的节点上创建 Pod。类似这种情况，可以指定 .spec.template.spec.affinity，之后 DaemonSet 控制器将在能够与节点亲和性匹配的节点上创建 Pod。 如果根本就没有指定，则 DaemonSet Controller 将在所有节点上创建 Pod。 Jobs 下面是一个 Job 配置示例。它负责计算 π 到小数点后 2000 位，并将结果打印出来。 此计算大约需要 10 秒钟完成。 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: perl command: [\"perl\", \"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"] restartPolicy: Never backoffLimit: 4 要查看 Job 对应的已完成的 Pods，可以执行 kubectl get pods。 要以机器可读的方式列举隶属于某 Job 的全部 Pods，你可以使用类似下面这条命令： pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}') echo $pods 输出类似于： pi-ntb4l 这里，选择算符与 Job 的选择算符相同。--output=jsonpath 选项给出了一个表达式， 用来从返回的列表中提取每个 Pod 的 name 字段。 查看其中一个 Pod 的标准输出： kubectl logs $pods 类似于： 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901 "},"worknote/kubernetes/service-resources.html":{"url":"worknote/kubernetes/service-resources.html","title":"Service资源","keywords":"","body":"Service 资源 创建和销毁 Kubernetes Pod 以匹配集群状态。 Pod 是非永久性资源。 如果你使用 Deployment 来运行你的应用程序，则它可以动态创建和销毁 Pod。 每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。 这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？ 于是就有了 Services。 定义 Service 例如，假定有一组 Pod，它们对外暴露了 6379 端口，同时还被打上 app=redis 标签： apiVersion: apps/v1 kind: Deployment metadata: name: redis-deployment labels: app: redis-deployment spec: replicas: 3 selector: matchLabels: app: redis template: metadata: labels: app: redis spec: containers: - name: redis image: redis ports: - containerPort: 6379 我们创建名称为 \"my-service\" 的 Service 对象，它会将请求代理到使用 TCP 端口 6379，并且具有标签 app=redis 的 Pod 上： apiVersion: v1 kind: Service metadata: name: my-service spec: selector: app: redis ports: - protocol: TCP port: 80 targetPort: 6379 通过执行 kubectl get endpoints my-service 命令可以看到 Service 后端所代理的 Pod： NAME ENDPOINTS AGE my-service 10.244.169.179:6379,10.244.169.180:6379,10.244.36.82:6379 6m3s Service 虚拟 IP 和服务代理 在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。 kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。简单来讲，一个 Service 对象就是工作节点上的一些 iptables 或 ipvs 规则，用于将到达 Service 对象 IP 地址的流量调度转发至相应的 Endpoint 对象指向的 IP 地址和端口之上。 userspace 代理模型 这种模式，kube-proxy 会监视 Kubernetes 控制平面对 Service 对象和 Endpoints 对象的添加和移除操作。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的后端 Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个后端 Pod，是 kube-proxy 基于 SessionAffinity 来确定的。 最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP） 和 Port 的请求，并重定向到代理端口，代理端口再代理请求到后端Pod。 默认情况下，用户空间模式下的 kube-proxy 通过轮转算法选择后端。 iptables 代理模型 这种模式，kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组后端中的某个 Pod 上面。 对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个后端组合。 默认的策略是，kube-proxy 在 iptables 模式下随机选择一个后端。 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理， 而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。 如果 kube-proxy 在 iptables 模式下运行，并且所选的第一个 Pod 没有响应， 则连接失败。 这与用户空间模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败， 并会自动使用其他后端 Pod 重试。 你可以使用 Pod 就绪探测器 验证后端 Pod 可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。 这样做意味着你避免将流量通过 kube-proxy 发送到已知已失败的 Pod。 ipvs 代理模型 在 ipvs 模式下，kube-proxy 监视 Kubernetes 服务和端点，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保IPVS 状态与所需状态匹配。访问服务时，IPVS 将流量定向到后端 Pod 之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数， 但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。 与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS 提供了更多选项来平衡后端 Pod 的流量。 这些是： rr: 轮替（Round-Robin）。 lc: 最少链接（Least Connection），即打开链接数量最少者优先。 dh: 目标地址哈希（Destination Hashing）。 sh: 源地址哈希（Source Hashing）。 sed: 最短预期延迟（Shortest Expected Delay）。 nq: 从不排队（Never Queue）。 Service 类型 对于一些应用的某些部分（如前端），可能希望将其暴露给 Kubernetes 集群外部的 IP 地址。 Kubernetes ServiceTypes 允许指定你所需要的 Service 类型，默认是 ClusterIP。 Type 的取值以及行为如下： ClusterIP: 通过集群的内部 IP 暴露服务，选择该值时服务只能够在集群内部访问。 这也是默认的 ServiceType。 NodePort: 通过每个节点上的 IP 和静态端口（NodePort）暴露服务。 NodePort 服务会路由到自动创建的 ClusterIP 服务。 通过请求 :，你可以从集群的外部访问一个 NodePort 服务。 LoadBalancer: 使用云提供商的负载均衡器向外部暴露服务。 外部负载均衡器可以将流量路由到自动创建的 NodePort 服务和 ClusterIP 服务上。 ExternalName: 通过返回 CNAME 和对应值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。 无需创建任何类型代理。 你也可以使用 Ingress 来暴露自己的服务。 Ingress 不是一种服务类型，但它充当集群的入口点。 它可以将路由规则整合到一个资源中，因为它可以在同一IP地址下公开多个服务。 NodePort 类型 如果你将 type 字段设置为 NodePort，则 Kubernetes 控制平面将在 --service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767）。 每个节点将那个端口（每个节点上的相同端口号）代理到你的服务中。 你的服务在其 .spec.ports[*].nodePort 字段中要求分配的端口。 如果需要特定的端口号，你可以在 nodePort 字段中指定一个值。 控制平面将为你分配该端口或报告 API 事务失败。 这意味着你需要自己注意可能发生的端口冲突。 你还必须使用有效的端口号，该端口号在配置用于 NodePort 的范围内。 使用 NodePort 可以让你自由设置自己的负载均衡解决方案， 配置 Kubernetes 不完全支持的环境， 甚至直接暴露一个或多个节点的 IP。 例如： apiVersion: v1 kind: Service metadata: name: my-service spec: type: NodePort selector: app: MyApp ports: # 默认情况下,为了方便起见,`targetPort` 被设置为与 `port` 字段相同的值 - port: 80 targetPort: 80 # 可选字段 # 默认情况下,为了方便起见,Kubernetes 控制平面会从某个范围内分配一个端口号(默认:30000-32767) nodePort: 30007 Service 启用 ipvs 代理模型 Kubernetes 的 1.11 版本后，默认使用 ipvs，如果节点的内核不支持或没有开启 ipvs 则 kubernetes 会自动降级为使用 iptables 规则。 查看 kube-proxy 的启动日志，这里默认使用的是 iptables 代理模型。 [root@k8s-master ~]$ kubectl logs -f kube-proxy-hjxcq -n kube-system ... I0323 03:36:31.452070 1 node.go:172] Successfully retrieved node IP: 10.10.110.192 I0323 03:36:31.452837 1 server_others.go:142] kube-proxy node IP is an IPv4 address (10.10.110.192), assume IPv4 operation W0323 03:36:31.577520 1 server_others.go:578] Unknown proxy mode \"\", assuming iptables proxy I0323 03:36:31.579263 1 server_others.go:185] Using iptables Proxier. I0323 03:36:31.580249 1 server.go:650] Version: v1.20.0 ... 在所有 Kubernetes 节点开启 ipvs 支持。 modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 查看内核模块是否加载。 lsmod | grep ip_vs 修改 ConfigMap 的 kube-system/kube-proxy 的配置文件为 ipvs。 kubectl edit configmaps kube-proxy -n kube-system ... ipvs: excludeCIDRs: null minSyncPeriod: 0s scheduler: \"\" strictARP: false syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s kind: KubeProxyConfiguration metricsBindAddress: \"\" mode: \"ipvs\" # 修改此处为ipvs ... 删除所有 kube-proxy 的 pod。 kubectl get pod -n kube-system | grep 'kube-proxy' | awk '{print $1}' | xargs -I {} kubectl delete pod {} -n kube-system 再次查看 kube-proxy 的日志。 [root@k8s-master ~]$ kubectl logs -f kube-proxy-pnglg -n kube-system ... I0410 09:05:52.455879 1 node.go:172] Successfully retrieved node IP: 10.10.110.192 I0410 09:05:52.459403 1 server_others.go:142] kube-proxy node IP is an IPv4 address (10.10.110.192), assume IPv4 operation I0410 09:05:52.599981 1 server_others.go:258] Using ipvs Proxier. W0410 09:05:52.606702 1 proxier.go:445] IPVS scheduler not specified, use rr by default I0410 09:05:52.608073 1 server.go:650] Version: v1.20.0 ... 查看 ipvs 相关规则。 apt install -y ipvsadm [root@k8s-master ~]$ ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -> RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 172.17.0.1:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 10.10.110.190:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 10.96.0.1:443 rr -> 10.10.110.190:6443 Masq 1 0 0 TCP 10.96.0.10:53 rr -> 10.244.36.73:53 Masq 1 0 0 -> 10.244.169.160:53 Masq 1 0 0 TCP 10.96.0.10:9153 rr -> 10.244.36.73:9153 Masq 1 0 0 -> 10.244.169.160:9153 Masq 1 0 0 TCP 10.97.105.36:80 rr TCP 10.98.47.133:443 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 10.110.221.168:8000 rr -> 10.244.169.159:8000 Masq 1 0 0 TCP 10.244.235.192:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 TCP 127.0.0.1:30023 rr -> 10.244.36.72:8443 Masq 1 0 0 UDP 10.96.0.10:53 rr -> 10.244.36.73:53 Masq 1 0 0 -> 10.244.169.160:53 Masq 1 0 0 "},"worknote/kubernetes/ingress.html":{"url":"worknote/kubernetes/ingress.html","title":"Ingress","keywords":"","body":"Ingress Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。 Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。 Ingress 是什么？ Ingress 公开了从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。 下面是一个将所有流量都发送到同一 Service 的简单 Ingress 示例： 可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL/TLS，以及提供基于名称的虚拟主机等能力。 Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。 Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或 Service.Type=LoadBalancer 类型的服务。 环境准备 你必须具有 Ingress 控制器 才能满足 Ingress 的要求。 仅创建 Ingress 资源本身没有任何效果。 你可能需要部署 Ingress 控制器，例如 ingress-nginx。 你可以从许多 Ingress 控制器 中进行选择。 理想情况下，所有 Ingress 控制器都应符合参考规范。但实际上，不同的 Ingress 控制器操作略有不同。 Ingress 资源 一个最小的 Ingress 资源示例： apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: minimal-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - path: /testpath pathType: Prefix backend: service: name: test port: number: 80 与所有其他 Kubernetes 资源一样，Ingress 需要使用 apiVersion、kind 和 metadata 字段。 Ingress 对象的命名必须是合法的 DNS 子域名名称。 有关使用配置文件的一般信息，请参见部署应用、 配置容器、 管理资源。 Ingress 经常使用注解（annotations）来配置一些选项，具体取决于 Ingress 控制器，例如 重写目标注解。 不同的 Ingress 控制器 支持不同的注解。查看文档以供你选择 Ingress 控制器，以了解支持哪些注解。 Ingress 规约 提供了配置负载均衡器或者代理服务器所需的所有信息。 最重要的是，其中包含与所有传入请求匹配的规则列表。 Ingress 资源仅支持用于转发 HTTP 流量的规则。 Ingress 规则 每个 HTTP 规则都包含以下信息： 可选的 host。在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 foo.bar.com），则 rules 适用于该 host。 路径列表 paths（例如，/testpath）,每个路径都有一个由 serviceName 和 servicePort 定义的关联后端。 在负载均衡器将流量定向到引用的服务之前，主机和路径都必须匹配传入请求的内容。 backend（后端）是 Service 文档 中所述的服务和端口名称的组合。 与规则的 host 和 path 匹配的对 Ingress 的 HTTP（和 HTTPS ）请求将发送到列出的 backend。 通常在 Ingress 控制器中会配置 defaultBackend（默认后端），以服务于任何不符合规约中 path 的请求。 DefaultBackend 没有 rules 的 Ingress 将所有流量发送到同一个默认后端。 defaultBackend 通常是 Ingress 控制器 的配置选项，而非在 Ingress 资源中指定。 如果 hosts 或 paths 都没有与 Ingress 对象中的 HTTP 请求匹配，则流量将路由到默认后端。 资源后端 Resource 后端是一个 ObjectRef，指向同一命名空间中的另一个 Kubernetes，将其作为 Ingress 对象。Resource 与 Service 配置是互斥的，在二者均被设置时会无法通过合法性检查。Resource 后端的一种常见用法是将所有入站数据导向带有静态资产的对象存储后端。 路径类型 Ingress 中的每个路径都需要有对应的路径类型（Path Type）。未明确设置 pathType 的路径无法通过合法性检查。当前支持的路径类型有三种： ImplementationSpecific: 对于这种路径类型，匹配方法取决于 IngressClass。 具体实现可以将其作为单独的 pathType 处理或者与 Prefix 或 Exact 类型作相同处理。 Exact: 精确匹配 URL 路径，且区分大小写。 Prefix: 基于以 / 分隔的 URL 路径前缀匹配。匹配区分大小写，并且对路径中的元素逐个完成。 路径元素指的是由 / 分隔符分隔的路径中的标签列表。 如果每个 p 都是请求路径 p 的元素前缀，则请求与路径 p 匹配。 说明： 如果路径的最后一个元素是请求路径中最后一个元素的子字符串，则不会匹配 （例如：/foo/bar 匹配 /foo/bar/baz, 但不匹配 /foo/barbaz）。 示例 类型 路径 请求路径 匹配与否？ Prefix / （所有路径） 是 Exact /foo /foo 是 Exact /foo /bar 否 Exact /foo /foo/ 否 Exact /foo/ /foo 否 Prefix /foo /foo, /foo/ 是 Prefix /foo/ /foo, /foo/ 是 Prefix /aaa/bb /aaa/bbb 否 Prefix /aaa/bbb /aaa/bbb 是 Prefix /aaa/bbb/ /aaa/bbb 是，忽略尾部斜线 Prefix /aaa/bbb /aaa/bbb/ 是，匹配尾部斜线 Prefix /aaa/bbb /aaa/bbb/ccc 是，匹配子路径 Prefix /aaa/bbb /aaa/bbbxyz 否，字符串前缀不匹配 Prefix /, /aaa /aaa/ccc 是，匹配 /aaa 前缀 Prefix /, /aaa, /aaa/bbb /aaa/bbb 是，匹配 /aaa/bbb 前缀 Prefix /, /aaa, /aaa/bbb /ccc 是，匹配 / 前缀 Prefix /aaa /ccc 否，使用默认后端 混合 /foo (Prefix), /foo (Exact) /foo 是，优选 Exact 类型 最基本的 Ingress 资源 注意：在此示例中，未指定 host，因此该规则适用于通过指定 IP 地址的所有入站 HTTP 通信。 如果提供了 host（例如 foo.bar.com），则 rules 适用于该 host。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - http: paths: - pathType: Prefix path: \"/nginx\" backend: service: name: nginx-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 基于 URL 路由代理服务 配置根据请求的 HTTP URI 将来自同一 IP 地址的流量路由到多个 Service。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: \"www.ingress.com\" http: paths: - pathType: Prefix path: \"/nginx\" backend: service: name: nginx-service port: number: 80 - pathType: Prefix path: \"/tomcat\" backend: service: name: tomcat-service port: number: 80 defaultBackend: # 如果hosts或paths都没有与Ingress对象中的HTTP请求匹配,则流量将路由到默认后端 service: name: default-http-backend port: number: 80 基于名称的虚拟托管 基于名称的虚拟主机支持将针对多个主机名的 HTTP 流量路由到同一 IP 地址上。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: rules: - host: \"www.nginx.com\" http: paths: - pathType: Prefix path: \"/\" backend: service: name: nginx-service port: number: 80 - host: \"www.tomcat.com\" http: paths: - pathType: Prefix path: \"/\" backend: service: name: tomcat-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 TLS 可以通过设定包含 TLS 私钥和证书的 Secret 来保护 Ingress。 Ingress 只支持单个 TLS 端口 443，并假定 TLS 连接终止于 Ingress 节点 （与 Service 及其 Pod 之间的流量都以明文传输）。 如果 Ingress 中的 TLS 配置部分指定了不同的主机，那么它们将根据通过 SNI TLS 扩展指定的主机名 （如果 Ingress 控制器支持 SNI）在同一端口上进行复用。 TLS Secret 必须包含名为 tls.crt 和 tls.key 的键名。 这些数据包含用于 TLS 的证书和私钥。例如： apiVersion: v1 kind: Secret metadata: name: testsecret-tls namespace: default data: tls.crt: base64 编码的 cert tls.key: base64 编码的 key type: kubernetes.io/tls 在 Ingress 中引用此 Secret 将会告诉 Ingress 控制器使用 TLS 加密从客户端到负载均衡器的通道。 你需要确保创建的 TLS Secret 创建自包含 https-example.foo.com 的公用名称（CN）的证书。 这里的公共名称也被称为全限定域名（FQDN）。 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: tls: - hosts: - www.missf.top secretName: www-missf-top rules: - host: www.missf.top http: paths: - pathType: Prefix path: \"/\" backend: service: name: nginx-service port: number: 80 defaultBackend: service: name: default-http-backend port: number: 80 生成 tls 自签证书 CFSSL 是 CloudFlare 开源的一款 PKI/TLS 工具。 CFSSL 包含一个命令行工具和一个用于签名、验证并且捆绑 TLS 证书的 HTTP API 服务，使用 Go 语言编写。 安裝 CFSSL 工具 curl -s -L -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 curl -s -L -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 curl -s -L -o /usr/local/bin/cfssl-certinfo https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 chmod +x /usr/local/bin/cfssl* 生成配置证书生成策略文件 cat > ca-config.json 这个策略，有一个默认的配置和一个 profile，这里的 profile 是 kubernetes。 signing: 表示该证书可用于签名其它证书。 server auth: 表示 client 可以用该 CA 对 server 提供的证书进行验证。 client auth: 表示 server 可以用该 CA 对 client 提供的证书进行验证。 expiry: 表示证书的有效期。 生成 CA 证书和私钥配置文件 cat > ca-csr.json CN: Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法。 C: Country， 国家。 ST: State，州，省。 L: Locality，地区，城市。 O: Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)。 OU: Organization Unit Name，组织单位名称，公司部门。 生成 CA 证书、CA 私钥、CSR 文件 cfssl gencert -initca ca-csr.json | cfssljson -bare ca - # 得到如下文件 [root@k8s-master ~/kubernetes/ssl]$ ll total 28 -rw-r--r-- 1 root root 294 Apr 17 15:46 ca-config.json -rw-r--r-- 1 root root 1013 Apr 17 15:48 ca.csr -rw-r--r-- 1 root root 274 Apr 17 15:47 ca-csr.json -rw------- 1 root root 1679 Apr 17 15:48 ca-key.pem -rw-r--r-- 1 root root 1387 Apr 17 15:48 ca.pem 生成服务端的证书信息 cat > www.missf.top-csr.json 使用 ca 证书签发证书 cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes www.missf.top-csr.json | cfssljson -bare www.missf.top 使用证书创建 secret 资源 kubectl create secret tls www-missf-top --cert=www.missf.top.pem --key=www.missf.top-key.pem "},"worknote/kubernetes/storage.html":{"url":"worknote/kubernetes/storage.html","title":"存储","keywords":"","body":"存储 Kubernetes 的存储方式有很多，具体可以到官网了解。 卷 Container 中的文件在磁盘上是临时存放的，这给 Container 中运行的较重要的应用程序带来一些问题。问题之一是当容器崩溃时文件丢失。kubelet 会重新启动容器， 但容器会以干净的状态重启。 第二个问题会在同一 Pod 中运行多个容器并共享文件时出现。 Kubernetes 卷（Volume）这一抽象概念能够解决这两个问题。 背景 Docker 也有 卷（Volume） 的概念，但对它只有少量且松散的管理。 Docker 卷是磁盘上或者另外一个容器内的一个目录。 Docker 提供卷驱动程序，但是其功能非常有限。 Kubernetes 支持很多类型的卷。 Pod 可以同时使用任意数目的卷类型。 临时卷类型的生命周期与 Pod 相同，但持久卷可以比 Pod 的存活期长。 因此，卷的存在时间会超出 Pod 中运行的所有容器，并且在容器重新启动时数据也会得到保留。 当 Pod 不再存在时，临时卷也将不再存在。但是持久卷会继续存在。 卷的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。 所采用的特定的卷类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。 使用卷时, 在 .spec.volumes 字段中设置为 Pod 提供的卷，并在 .spec.containers[*].volumeMounts 字段中声明卷在容器中的挂载位置。 容器中的进程看到的是由它们的 Docker 镜像和卷组成的文件系统视图。 Docker 镜像 位于文件系统层次结构的根部。各个卷则挂载在镜像内的指定路径上。 卷不能挂载到其他卷之上，也不能与其他卷有硬链接。 Pod 配置中的每个容器必须独立指定各个卷的挂载位置。 卷类型 Kubernetes 支持很多类型的卷，下面主要列举一些常用的卷。 emptyDir 当 Pod 分派到某个 Node 上时，emptyDir 卷会被创建，并且在 Pod 在该节点上运行期间，卷一直存在。 就像其名称表示的那样，卷最初是空的。 尽管 Pod 中的容器挂载 emptyDir 卷的路径可能相同也可能不同，这些容器都可以读写 emptyDir 卷中相同的文件。 当 Pod 因为某些原因被从节点上删除时，emptyDir 卷中的数据也会被永久删除。 说明： 容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃期间 emptyDir 卷中的数据是安全的。 取决于你的环境，emptyDir 卷存储在该节点所使用的介质上；这里的介质可以是磁盘或 SSD 或网络存储。但是，你可以将 emptyDir.medium 字段设置为 \"Memory\"，以告诉 Kubernetes 为你挂载 tmpfs（基于 RAM 的文件系统）。 虽然 tmpfs 速度非常快，但是要注意它与磁盘不同。 tmpfs 在节点重启时会被清除，并且你所写入的所有文件都会计入容器的内存消耗，受容器内存限制约束。 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-containers image: ikubernetes/myapp:v1 volumeMounts: # 容器的/usr/share/nginx/html目录挂载到emptyDir卷 - mountPath: /usr/share/nginx/html name: html - name: busybox image: busybox volumeMounts: # 容器的/data目录挂载到emptyDir卷 - mountPath: /data name: html command: [\"/bin/sh\", \"-c\", \"while true; do echo $(date) >> /data/index.html; sleep 2;done\"] volumes: # 在pod所在节点定义emptyDir卷 - name: html emptyDir: {} # 同一pod的两个不同的容器挂载到同一个emptyDir卷,实现数据共享和数据交互 hostPath hostPath 卷能将主机节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。 除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。 支持的 type 值如下： 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。 DirectoryOrCreate 如果在给定路径上什么都不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 kubelet 相同的组和属主信息。 Directory 在给定路径上必须存在的目录。 FileOrCreate 如果在给定路径上什么都不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 kubelet 相同的组和所有权。 File 在给定路径上必须存在的文件。 Socket 在给定路径上必须存在的 UNIX 套接字。 CharDevice 在给定路径上必须存在的字符设备。 BlockDevice 在给定路径上必须存在的块设备。 当使用这种类型的卷时要小心，因为： 具有相同配置（例如基于同一 PodTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为。 下层主机上创建的文件或目录只能由 root 用户写入。你需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。 apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /usr/share/nginx/html # 容器里目录位置 name: html volumes: - name: html hostPath: path: /data # 宿主上目录位置 type: DirectoryOrCreate nfs nfs 卷能将 NFS (网络文件系统) 挂载到你的 Pod 中。 不像 emptyDir 那样会在删除 Pod 的同时也会被删除，nfs 卷的内容在删除 Pod 时会被保存，卷只是被卸载。 这意味着 nfs 卷可以被预先填充数据，并且这些数据可以在 Pod 之间共享。 注意： 在使用 NFS 卷之前，你必须运行自己的 NFS 服务器并将目标 share 导出备用。 部署 NFS 服务端： apt-get update apt-get install nfs-kernel-server -y mkdir /mnt/nfs/ vim /etc/exports /mnt/nfs 10.10.110.0/24(rw,no_root_squash,no_subtree_check) # 将NFS服务端的/mnt/nfs目录share出去 systemctl restart nfs-server.service 注意： NFS 服务端和客户端之间需要关闭防火墙，Kubernetes 节点必须支持驱动 NFS 存储设备。 客户端（Kubernetes 所有工作节点）： apt-get update apt-get install nfs-common -y # 客户端连接NFS服务器所需的包 mount -t nfs nfs:/mnt/nfs /mnt/nfs # 将nfs节点的/mnt/nfs目录挂载到本地节点的/mnt/nfs目录,创建文件测试两个节点的目录是否共享 创建 NFS 存储类的资源配置清单： apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: nginx image: nginx volumeMounts: - mountPath: /usr/share/nginx/html name: html volumes: - name: html nfs: path: /mnt/nfs server: www.nfs.com 注意：我们的 Kubernetes 工作节点不需要提前挂载 NFS 卷，创建 pod 时会自动挂载，pod 销毁之后 NFS 卷也会自动卸载。 运行 pod 之后，我们可以在 pod 所在节点查看 NFS 挂载情况： df -h | grep nfs nfs:/mnt/nfs 19G 4.5G 14G 26% /var/lib/kubelet/pods/0a87a303-9e44-4388-9e89-4bf848d159cf/volumes/kubernetes.io~nfs/html # nfs:/mnt/nfs 这是pod所在节点挂载NFS服务的目录 # /var/lib/kubelet/pods/0a87a303-9e44-4388-9e89-4bf848d159cf/volumes/kubernetes.io~nfs/html 是pod内/usr/share/nginx/html目录在宿主机上的映射 这时候我们在 NFS 服务器修改 NFS 共享目录 /mnt/nfs ，数据会同步到 pod 内的 /usr/share/nginx/html 目录。 persistentVolumeClaim persistentVolumeClaim 卷用来将持久卷（PersistentVolume） 挂载到 Pod 中。 持久卷申领（PersistentVolumeClaim）是用户在不知道特定云环境细节的情况下\"申领\"持久存储 （例如 GCE PersistentDisk 或者 iSCSI 卷）的一种方法。 持久卷（PersistentVolume，PV）是集群中的一块存储，可以由管理员事先供应，或者使用存储类（Storage Class）来动态供应。 持久卷是集群资源，就像节点也是集群资源一样。PV 持久卷和普通的 Volume 一样，也是使用卷插件来实现的，只是它们拥有独立于任何使用 PV 的 Pod 的生命周期。 此 API 对象中记述了存储的实现细节，无论其背后是 NFS、iSCSI 还是特定于云平台的存储系统。 持久卷申领（PersistentVolumeClaim，PVC）表达的是用户对存储的请求。概念上与 Pod 类似。 Pod 会耗用节点资源，而 PVC 申领会耗用 PV 资源。Pod 可以请求特定数量的资源（CPU 和内存）；同样 PVC 申领也可以请求特定的大小和访问模式 （例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载）。 每个 PV 对象都包含 spec 部分和 status 部分，分别对应卷的规约和状态： --- apiVersion: v1 kind: PersistentVolume metadata: name: pv1 spec: capacity: storage: 1Gi volumeMode: Filesystem accessModes: - ReadWriteOnce - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv1 server: www.nfs.com --- apiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: capacity: storage: 2Gi volumeMode: Filesystem accessModes: - ReadOnlyMany - ReadWriteOnce persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv2 server: www.nfs.com --- apiVersion: v1 kind: PersistentVolume metadata: name: pv3 spec: capacity: storage: 3Gi volumeMode: Filesystem accessModes: - ReadWriteMany - ReadWriteOnce - ReadOnlyMany persistentVolumeReclaimPolicy: Retain nfs: path: /mnt/pv3 server: www.nfs.com 一般而言，每个 PV 卷都有确定的存储容量。 容量属性是使用 PV 对象的 capacity 属性来设置的。 目前，存储大小是可以设置和请求的唯一资源。 未来可能会包含 IOPS、吞吐量等属性。 针对 PV 持久卷，Kuberneretes 支持两种卷模式（volumeModes）：Filesystem（文件系统） 和 Block（块）。 volumeMode 是一个可选的 API 参数。 如果该参数被省略，默认的卷模式是 Filesystem。 volumeMode 属性设置为 Filesystem 的卷会被 Pod 挂载（Mount）到某个目录。 如果卷的存储来自某块设备而该设备目前为空，Kuberneretes 会在第一次挂载卷之前在设备上创建文件系统。 你可以将 volumeMode 设置为 Block，以便将卷作为原始块设备来使用。 这类卷以块设备的方式交给 Pod 使用，其上没有任何文件系统。 这种模式对于为 Pod 提供一种使用最快可能方式来访问卷而言很有帮助，Pod 和卷之间不存在文件系统层。另外，Pod 中运行的应用必须知道如何处理原始块设备。 关于如何在 Pod 中使用 volumeMode: Block 的卷，可参阅 原始块卷支持。 PersistentVolume 卷可以用资源提供者所支持的任何方式挂载到宿主系统上。 如下表所示，提供者（驱动）的能力不同，每个 PV 卷的访问模式都会设置为对应卷所支持的模式值。 例如，NFS 可以支持多个读写客户，但是某个特定的 NFS PV 卷可能在服务器上以只读的方式导出。每个 PV 卷都会获得自身的访问模式集合，描述的是特定 PV 卷的能力。 访问模式有： ReadWriteOnce -- 卷可以被一个节点以读写方式挂载。 ReadOnlyMany -- 卷可以被多个节点以只读方式挂载。 ReadWriteMany -- 卷可以被多个节点以读写方式挂载。 在命令行接口（CLI）中，访问模式也使用以下缩写形式： RWO - ReadWriteOnce。 ROX - ReadOnlyMany。 RWX - ReadWriteMany。 目前的回收策略有： Retain -- 手动回收（pvc 被删除后，pv 还保留着数据，只是 pv 的状态变为 Released ，并且 pv 不能再次被 pvc 绑定）。 Recycle -- 基本擦除 (pvc 被删除后，pv 不保留数据，pv 可以再次被 pvc 绑定， rm -rf /thevolume/*)。 Delete -- 诸如 AWS EBS、GCE PD、Azure Disk 或 OpenStack Cinder 卷这类关联存储资产也被删除。 目前，仅 NFS 和 HostPath 支持回收（Recycle）。 AWS EBS、GCE PD、Azure Disk 和 Cinder 卷都支持删除（Delete）。 每个 PVC 对象都有 spec 和 status 部分，分别对应申领的规约和状态： --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi 申领在请求具有特定访问模式的存储时，使用与卷相同的访问模式约定。 申领使用与卷相同的约定来表明是将卷作为文件系统还是块设备来使用。 申领和 Pod 一样，也可以请求特定数量的资源。在这个上下文中，请求的资源是存储。 卷和申领都使用相同的 资源模型。 申领可以设置标签选择算符来进一步过滤卷集合。只有标签与选择算符相匹配的卷能够绑定到申领上。 选择算符包含两个字段： matchLabels - 卷必须包含带有此值的标签。 matchExpressions - 通过设定键（key）、值列表和操作符（operator） 来构造的需求。合法的操作符有 In、NotIn、Exists 和 DoesNotExist。 定义 PersistentVolume 的标签： kubectl get persistentvolume --show-labels NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE LABELS pv1 1Gi RWO,RWX Retain Available default/pvc1 43m release=stable pv2 2Gi RWO,ROX Retain Available 43m pv3 3Gi RWO,ROX,RWX Retain Available 43m PersistentVolumeClaim 通过 PersistentVolume 的标签去进行绑定： --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: default spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi selector: matchLabels: release: \"stable\" matchExpressions: - {key: release, operator: In, values: [stable]} 存储类 StorageClass 为管理员提供了描述存储 \"类\" 的方法。 不同的类型可能会映射到不同的服务质量等级或备份策略，或是由集群管理员制定的任意策略。 Kubernetes 本身并不清楚各种类代表的什么。这个类的概念在其他存储系统中有时被称为 \"配置文件\"。 StorageClass 资源 每个 StorageClass 都包含 provisioner、parameters 和 reclaimPolicy 字段， 这些字段会在 StorageClass 需要动态分配 PersistentVolume 时会使用到。 StorageClass 对象的命名很重要，用户使用这个命名来请求生成一个特定的类。 当创建 StorageClass 对象时，管理员设置 StorageClass 对象的命名和其他参数，一旦创建了对象就不能再对其更新。 存储制备器 每个 StorageClass 都有一个制备器（Provisioner），用来决定使用哪个卷插件制备 PV。 该字段必须指定。 卷插件 内置制备器 配置例子 AWSElasticBlockStore ✓ AWS EBS AzureFile ✓ Azure File AzureDisk ✓ Azure Disk CephFS - - Cinder ✓ OpenStack Cinder FC - - FlexVolume - - Flocker ✓ - GCEPersistentDisk ✓ GCE PD Glusterfs ✓ Glusterfs iSCSI - - Quobyte ✓ Quobyte NFS - - RBD ✓ Ceph RBD VsphereVolume ✓ vSphere PortworxVolume ✓ Portworx Volume ScaleIO ✓ ScaleIO StorageOS ✓ StorageOS Local - Local 动态卷供应 动态卷供应允许按需创建存储卷。 如果没有动态供应，集群管理员必须手动地联系他们的云或存储提供商来创建新的存储卷， 然后在 Kubernetes 集群创建PersistentVolume 对象来表示这些卷。 动态供应功能消除了集群管理员预先配置存储的需要。 相反，它在用户请求时自动供应存储。 由于 NFS 卷插件并不支持内置制备器，所以我们用 NFS 作为底层存储去配置动态卷供应时，需要使用第三方的 NFS 插件 nfs-subdir-external-provisioner。 部署 NFS 插件： apiVersion: v1 kind: Namespace metadata: name: nfs-subdir-external-provisioner labels: name: nfs-subdir-external-provisioner --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: managed-nfs-storage provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME' parameters: archiveOnDelete: \"false\" # \"false\": 删除pvc之后NFS存储后端不会保留数据目录,\"true\": 删除pvc之后NFS存储后端会保留数据目录 --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner # replace with namespace where provisioner is deployed namespace: nfs-subdir-external-provisioner spec: replicas: 1 strategy: type: Recreate selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: k8s-sigs.io/nfs-subdir-external-provisioner - name: NFS_SERVER value: 10.10.110.23 # NFS的服务地址 - name: NFS_PATH value: /data/kubernetes # NFS的Export路径 volumes: - name: nfs-client-root nfs: server: 10.10.110.23 path: /data/kubernetes 创建 PersistentVolumeClaim 测试动态卷供应： # 指定storageClassName --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi storageClassName: \"managed-nfs-storage\" # 使用annotations --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\" spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi # 指定sc作为默认存储后端 kubectl patch storageclass managed-nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 spec: accessModes: - ReadWriteOnce volumeMode: Filesystem resources: requests: storage: 800Mi 说明：关于 kubernetes 1.20 版本使用 NFS 插件出现 unexpected error getting claim reference: selfLink was empty, can't make reference 的报错。这是因为 kubernetes 1.20 版本禁用了 selfLink 。解决方法： vim /etc/kubernetes/manifests/kube-apiserver.yaml ... spec: containers: - command: - kube-apiserver - --feature-gates=RemoveSelfLink=false # 添加这一行 ... "},"worknote/kubernetes/configuration.html":{"url":"worknote/kubernetes/configuration.html","title":"配置","keywords":"","body":"配置 此模块整合了 Kubernetes 所有资源的配置。 配置最佳实践 https://kubernetes.io/zh/docs/concepts/configuration/overview/ ConfigMap ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将您的环境配置信息和 容器镜像 解耦，便于应用配置的修改。 注意：ConfigMap 并不提供保密或者加密功能。 如果你想存储的数据是机密的，请使用 Secret， 或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。 动机 使用 ConfigMap 来将你的配置数据和应用程序代码分开。 比如，假设你正在开发一个应用，它可以在你自己的电脑上（用于开发）和在云上 （用于实际流量）运行。 你的代码里有一段是用于查看环境变量 DATABASE_HOST，在本地运行时， 你将这个变量设置为 localhost，在云上，你将其设置为引用 Kubernetes 集群中的公开数据库组件的 服务。 这让你可以获取在云中运行的容器镜像，并且如果有需要的话，在本地调试完全相同的代码。 ConfigMap 在设计上不是用来保存大量数据的。在 ConfigMap 中保存的数据不可超过 1 MiB。如果你需要保存超出此尺寸限制的数据，你可能希望考虑挂载存储卷或者使用独立的数据库或者文件服务。 ConfigMap 对象 ConfigMap 是一个 API 对象， 让你可以存储其他对象所需要使用的配置。 和其他 Kubernetes 对象都有一个 spec ，不同的是，ConfigMap 使用 data 和 binaryData 字段。这些字段能够接收键-值对作为其取值。data 和 binaryData 字段都是可选的。data 字段设计用来保存 UTF-8 字节序列，而 binaryData 则被设计用来保存二进制数据作为 base64 编码的字串。 ConfigMap 的名字必须是一个合法的 DNS 子域名。 data 或 binaryData 字段下面的每个键的名称都必须由字母数字字符或者 -、_ 或 . 组成。在 data 下保存的键名不可以与在 binaryData 下出现的键名有重叠。 从 v1.19 开始，你可以添加一个 immutable 字段到 ConfigMap 定义中，创建 不可变更的 ConfigMap。 ConfigMaps 和 Pods 你可以写一个引用 ConfigMap 的 Pod 的 spec，并根据 ConfigMap 中的数据在该 Pod 中配置容器。这个 Pod 和 ConfigMap 必须要在同一个 名字空间 中。 这是一个 ConfigMap 的示例，它的一些键只有一个值，其他键的值看起来像是配置的片段格式。 apiVersion: v1 kind: ConfigMap metadata: name: configmap-demo data: # 类属性键:每一个键都映射到一个简单的值 nginx_server_port: \"80\" nginx_server_host: \"www.missf.top\" # 类文件键 nginx.config: | location / { root /usr/local/nginx/html/; index index.html; } 你可以使用四种方式来使用 ConfigMap 配置 Pod 中的容器： 在容器命令和参数内。 容器的环境变量。 在只读卷里面添加一个文件，让应用来读取。 编写代码在 Pod 中运行，使用 Kubernetes API 来读取 ConfigMap。 这些不同的方法适用于不同的数据使用方式。 对前三个方法，kubelet 使用 ConfigMap 中的数据在 Pod 中启动容器。 第四种方法意味着你必须编写代码才能读取 ConfigMap 和它的数据。然而， 由于你是直接使用 Kubernetes API，因此只要 ConfigMap 发生更改，你的应用就能够通过订阅来获取更新，并且在这样的情况发生的时候做出反应。 通过直接进入 Kubernetes API，这个技术也可以让你能够获取到不同的名字空间里的 ConfigMap。 下面是一个 Pod 的示例，它通过使用 configmap-demo 中的值来配置一个 Pod: apiVersion: v1 kind: Pod metadata: name: configmap-pod labels: app: configmap spec: containers: - name: nginx image: nginx ports: - containerPort: 80 env: - name: NGINX_SERVER_PORT valueFrom: configMapKeyRef: name: configmap-demo key: nginx_server_port - name: NGINX_SERVER_HOST valueFrom: configMapKeyRef: name: configmap-demo key: nginx_server_host volumeMounts: - name: config mountPath: \"/config\" readOnly: true volumes: - name: config configMap: name: configmap-demo items: - key: \"nginx.config\" path: \"nginx.config\" ConfigMap 不会区分单行属性值和多行类似文件的值，重要的是 Pods 和其他对象如何使用这些值。（能否自动更新也是看 Pods 如何去使用 ConfigMap）。 上面的例子定义了一个卷并将它作为 /config 文件夹挂载到 demo 容器内， 创建一个文件，/config/nginx.config。 使用 ConfigMap ConfigMap 可以作为数据卷挂载。ConfigMap 也可被系统的其他组件使用，而不一定直接暴露给 Pod。例如，ConfigMap 可以保存系统中其他组件要使用的配置数据。 ConfigMap 最常见的用法是为同一命名空间里某 Pod 中运行的容器执行配置。 你也可以单独使用 ConfigMap。 比如，你可能会遇到基于 ConfigMap 来调整其行为的 插件 或者 operator。 在 Pod 中将 ConfigMap 当做文件使用 创建一个 ConfigMap 对象或者使用现有的 ConfigMap 对象。多个 Pod 可以引用同一个 ConfigMap。 修改 Pod 定义，在 spec.volumes[] 下添加一个卷。 为该卷设置任意名称，之后将 spec.volumes[].configMap.name 字段设置为对你的 ConfigMap 对象的引用。 为每个需要该 ConfigMap 的容器添加一个 .spec.containers[].volumeMounts[]。 设置 .spec.containers[].volumeMounts[].readOnly=true 并将 .spec.containers[].volumeMounts[].mountPath 设置为一个未使用的目录名， ConfigMap 的内容将出现在该目录中。 更改你的镜像或者命令行，以便程序能够从该目录中查找文件。ConfigMap 中的每个 data 键会变成 mountPath 下面的一个文件名。 下面是一个将 ConfigMap 以卷的形式进行挂载的 Pod 示例： apiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mypod image: redis volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo configMap: name: myconfigmap 你希望使用的每个 ConfigMap 都需要在 spec.volumes 中被引用到。 如果 Pod 中有多个容器，则每个容器都需要自己的 volumeMounts 块，但针对每个 ConfigMap，你只需要设置一个 spec.volumes 块。 被挂载的 ConfigMap 内容会被自动更新 当卷中使用的 ConfigMap 被更新时，所投射的键最终也会被更新。 kubelet 组件会在每次周期性同步时检查所挂载的 ConfigMap 是否为最新。 不过，kubelet 使用的是其本地的高速缓存来获得 ConfigMap 的当前值。 高速缓存的类型可以通过 KubeletConfiguration 结构 的 ConfigMapAndSecretChangeDetectionStrategy 字段来配置。 ConfigMap 既可以通过 watch 操作实现内容传播（默认形式），也可实现基于 TTL 的缓存，还可以直接经过所有请求重定向到 API 服务器。 因此，从 ConfigMap 被更新的那一刻算起，到新的主键被投射到 Pod 中去，这一 时间跨度可能与 kubelet 的同步周期加上高速缓存的传播延迟相等。 这里的传播延迟取决于所选的高速缓存类型 （分别对应 watch 操作的传播延迟、高速缓存的 TTL 时长或者 0）。 以环境变量方式使用的 ConfigMap 数据不会被自动更新。 更新这些数据需要重新启动 Pod。 不可变更的 ConfigMap Kubernetes 不可变更的 Secret 和 ConfigMap 提供了一种将各个 Secret 和 ConfigMap 设置为不可变更的选项。对于大量使用 ConfigMap 的集群（至少有数万个各不相同的 ConfigMap 给 Pod 挂载）而言，禁止更改 ConfigMap 的数据有以下好处： 保护应用，使之免受意外（不想要的）更新所带来的负面影响。 通过大幅降低对 kube-apiserver 的压力提升集群性能，这是因为系统会关闭对已标记为不可变更的 ConfigMap 的监视操作。 此功能特性由 ImmutableEphemeralVolumes 特性门控 来控制。你可以通过将 immutable 字段设置为 true 创建不可变更的 ConfigMap。 例如： apiVersion: v1 kind: ConfigMap metadata: ... data: ... immutable: true 一旦某 ConfigMap 被标记为不可变更，则无法逆转这一变化，也无法更改 data 或 binaryData 字段的内容。你只能删除并重建 ConfigMap。 因为现有的 Pod 会维护一个对已删除的 ConfigMap 的挂载点，建议重新创建这些 Pods。 Secret Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 参阅 Secret 设计文档 获取更多详细信息。 Secret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 用户可以创建 Secret，同时系统也创建了一些 Secret。 注意：Kubernetes Secret 默认情况下存储为 base64-编码的、非加密的字符串。 默认情况下，能够访问 API 的任何人，或者能够访问 Kubernetes 下层数据存储（etcd） 的任何人都可以以明文形式读取这些数据。 为了能够安全地使用 Secret，我们建议你（至少）： 为 Secret 启用静态加密； 启用或配置 RBAC 规则来限制对 Secret 的读写操作。 要注意，任何被允许创建 Pod 的人都默认地具有读取 Secret 的权限。 Secret 概览 要使用 Secret，Pod 需要引用 Secret。 Pod 可以用三种方式之一来使用 Secret: 作为挂载到一个或多个容器上的 卷 中的文件。 作为容器的环境变量 由 kubelet 在为 Pod 拉取镜像时使用 Secret 对象的名称必须是合法的 DNS 子域名。 在为创建 Secret 编写配置文件时，你可以设置 data 与/或 stringData 字段。 data 和 stringData 字段都是可选的。data 字段中所有键值都必须是 base64 编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置 stringData 字段，其中可以使用任何字符串作为其取值。 Secret 的类型 在创建 Secret 对象时，你可以使用 Secret 资源的 type 字段，或者与其等价的 kubectl 命令行参数（如果有的话）为其设置类型。 Secret 的类型用来帮助编写程序处理 Secret 数据。 Kubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。 内置类型 用法 Opaque 用户定义的任意数据 kubernetes.io/service-account-token 服务账号令牌 kubernetes.io/dockercfg ~/.dockercfg 文件的序列化形式 kubernetes.io/dockerconfigjson ~/.docker/config.json 文件的序列化形式 kubernetes.io/basic-auth 用于基本身份认证的凭据 kubernetes.io/ssh-auth 用于 SSH 身份认证的凭据 kubernetes.io/tls 用于 TLS 客户端或者服务器端的数据 bootstrap.kubernetes.io/token 启动引导令牌数据 通过为 Secret 对象的 type 字段设置一个非空的字符串值，你也可以定义并使用自己 Secret 类型。如果 type 值为空字符串，则被视为 Opaque 类型。 Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一， 则你必须满足为该类型所定义的所有要求。 创建 Secret 有几种不同的方式来创建 Secret: 使用 kubectl 命令创建 Secret 一个 Secret 可以包含 Pod 访问数据库所需的用户凭证。 例如，由用户名和密码组成的数据库连接字符串。 你可以在本地计算机上，将用户名存储在文件 ./username.txt 中，将密码存储在文件 ./password.txt 中。 echo -n 'admin' > ./username.txt echo -n 'Er34ff5ghoo' > ./password.txt 在这些命令中，-n 标志确保生成的文件在文本末尾不包含额外的换行符。 这一点很重要，因为当 kubectl 读取文件并将内容编码为 base64 字符串时，多余的换行符也会被编码。 kubectl create secret 命令将这些文件打包成一个 Secret 并在 API 服务器上创建对象。 kubectl create secret generic db-user-pass --from-file=user=username.txt --from-file=pass=password.txt 输出类似于： secret/db-user-pass created 默认密钥名称是文件名。 你可以选择使用 --from-file=[key=]source 来设置密钥名称。例如： kubectl create secret generic db-user-pass \\ --from-file=username=./username.txt \\ --from-file=password=./password.txt 检查 secret 是否已创建： kubectl get secrets 你可以查看 Secret 的描述： kubectl describe secrets/db-user-pass 要查看创建的 Secret 的内容，运行以下命令： kubectl get secret db-user-pass -o jsonpath='{.data}' 输出类似于： {\"pass\":\"RXIzNGZmNWdob28=\",\"user\":\"YWRtaW4=\"} 现在你可以解码 pass 的数据： echo \"RXIzNGZmNWdob28=\" | base64 --decode 输出类似于： Er34ff5ghoo 使用配置文件来创建 Secret 你可以先用 JSON 或 YAML 格式在文件中创建 Secret，然后创建该对象。 Secret 资源包含2个键值对： data 和 stringData。 data 字段用来存储 base64 编码的任意数据。 提供 stringData 字段是为了方便，它允许 Secret 使用未编码的字符串。 data 和 stringData 的键必须由字母、数字、-，_ 或 . 组成。 例如，要使用 Secret 的 data 字段存储两个字符串，请将字符串转换为 base64 ，如下所示： echo -n 'admin' | base64 echo -n 'Er34ff5ghoo' | base64 输出类似于： YWRtaW4= RXIzNGZmNWdob28= 编写一个 Secret 配置文件，如下所示： apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: RXIzNGZmNWdob28= 对于某些场景，你可能希望使用 stringData 字段。 这字段可以将一个非 base64 编码的字符串直接放入 Secret 中， 当创建或更新该 Secret 时，此字段将被编码。 例如，如果你的应用程序使用以下配置文件： apiUrl: \"https://my.api.com/api/v1\" username: \"\" password: \"\" 你可以使用以下定义将其存储在 Secret 中： apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque stringData: config.yaml: | apiUrl: \"https://my.api.com/api/v1\" username: password: 使用 kustomize 来创建 Secret 你可以在 kustomization.yaml 中定义 secreteGenerator，并在定义中引用其他现成的文件，生成 Secret。 例如：下面的 kustomization 文件 引用了 ./username.txt 和 ./password.txt 文件： secretGenerator: - name: db-user-pass files: - username.txt - password.txt 你也可以在 kustomization.yaml 文件中指定一些字面量定义 secretGenerator。 例如：下面的 kustomization.yaml 文件中包含了 username 和 password 两个字面量： secretGenerator: - name: db-user-pass literals: - username=admin - password=1f2d1e2e67df 注意，上面两种情况，你都不需要使用 base64 编码。 使用 kubectl apply 命令应用包含 kustomization.yaml 文件的目录创建 Secret。 kubectl apply -k . 编辑 Secret 你可以通过下面的命令编辑现有的 Secret: kubectl edit secrets mysecret 使用 Secret Secret 可以作为数据卷被挂载，或作为 环境变量 暴露出来以供 Pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 Pod 内。 例如，它们可以保存凭据，系统的其他部分将用它来代表你与外部系统进行交互。 将 Secret 作为 Pod 中的环境变量使用： 创建一个 Secret 或者使用一个已存在的 Secret。多个 Pod 可以引用同一个 Secret。 修改 Pod 定义，为每个要使用 Secret 的容器添加对应 Secret 键的环境变量。 使用 Secret 键的环境变量应在 env[x].valueFrom.secretKeyRef 中指定要包含的 Secret 名称和键名。 更改镜像并／或者命令行，以便程序在指定的环境变量中查找值。 这是一个使用来自环境变量中的 Secret 值的 Pod 示例： apiVersion: v1 kind: Pod metadata: name: secret-env-pod spec: containers: - name: nginx image: nginx env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 在 Pod 中使用存放在卷中的 Secret: 创建一个 Secret 或者使用已有的 Secret。多个 Pod 可以引用同一个 Secret。 修改你的 Pod 定义，在 spec.volumes[] 下增加一个卷。可以给这个卷随意命名， 它的 spec.volumes[].secret.secretName 必须是 Secret 对象的名字。 将 spec.containers[].volumeMounts[] 加到需要用到该 Secret 的容器中。 指定 spec.containers[].volumeMounts[].readOnly = true 和 spec.containers[].volumeMounts[].mountPath 为你想要该 Secret 出现的尚未使用的目录。 修改你的镜像并且／或者命令行，让程序从该目录下寻找文件。 Secret 的 data 映射中的每一个键都对应 mountPath 下的一个文件名。 这是一个在 Pod 中使用存放在挂载卷中 Secret 的例子： apiVersion: v1 kind: Pod metadata: name: secret-volume-pod spec: containers: - name: nginx image: nginx volumeMounts: - name: foo mountPath: \"/etc/foo\" readOnly: true volumes: - name: foo secret: secretName: mysecret 挂载的 Secret 会被自动更新 当已经存储于卷中被使用的 Secret 被更新时，被映射的键也将被更新。 组件 kubelet 在周期性同步时检查被挂载的 Secret 是不是最新的。 但是，它会使用其本地缓存的数值作为 Secret 的当前值。 "},"worknote/kubernetes/statefulset.html":{"url":"worknote/kubernetes/statefulset.html","title":"StatefulSet","keywords":"","body":"StatefulSet StatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 用来管理某 Pod 集合的部署和扩缩， 并为这些 Pod 提供持久存储和持久标识符。 和 Deployment 类似， StatefulSet 管理基于相同容器规约的一组 Pod。但和 Deployment 不同的是， StatefulSet 为它们的每个 Pod 维护了一个有粘性的 ID。这些 Pod 是基于相同的规约来创建的， 但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值： 稳定的、唯一的网络标识符。 稳定的、持久的存储。 有序的、优雅的部署和缩放。 有序的、自动的滚动更新。 在上面描述中，“稳定的”意味着 Pod 调度或重调度的整个过程是有持久性的。 如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如 Deployment 或者 ReplicaSet 可能更适用于你的无状态应用部署需要。 创建 StatefulSet 作为开始，使用如下示例创建一个 StatefulSet。它和 StatefulSets 概念中的示例相似。 它创建了一个 Headless Service statefulset-service 用来发布 StatefulSet web 中的 Pod 的 IP 地址。 apiVersion: v1 kind: Service metadata: name: statefulset-service namespace: default labels: app: statefulset spec: type: ClusterIP clusterIP: None ports: - port: 80 protocol: TCP targetPort: 80 selector: app: nginx --- apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"statefulset-service\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: storageClassName: \"managed-nfs-storage\" accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 2Gi 顺序创建 Pod 对于一个拥有 N 个副本的 StatefulSet，Pod 被部署时是按照 {0 …… N-1} 的序号顺序创建的。 在第一个终端中使用 kubectl get 检查输出。这个输出最终将看起来像下面的样子。 kubectl get pod -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 0/1 Pending 0 0s web-0 0/1 Pending 0 0s web-0 0/1 ContainerCreating 0 0s web-0 0/1 ContainerCreating 0 2s web-0 1/1 Running 0 18s web-1 0/1 Pending 0 0s web-1 0/1 Pending 0 0s web-1 0/1 ContainerCreating 0 0s web-1 0/1 ContainerCreating 0 1s web-1 1/1 Running 0 17s web-2 0/1 Pending 0 0s web-2 0/1 Pending 0 0s web-2 0/1 ContainerCreating 0 0s web-2 0/1 ContainerCreating 0 1s web-2 1/1 Running 0 17s 请注意在 web-0 Pod 处于 Running和Ready 状态后 web-1 Pod 才会被启动。 StatefulSet 中的 Pod StatefulSet 中的 Pod 拥有一个唯一的顺序索引和稳定的网络身份标识。 检查 Pod 的顺序索引 获取 StatefulSet 的 Pod。 kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 5m11s web-1 1/1 Running 0 4m53s web-2 1/1 Running 0 4m36s 如同 StatefulSets 概念中所提到的， StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标志。 这个标志基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。 Pod 的名称的形式为-。 webStatefulSet 拥有两个副本，所以它创建了三个 Pod: web-0，web-1 和 web-2。 使用稳定的网络身份标识 每个 Pod 都拥有一个基于其顺序索引的稳定的主机名。使用kubectl exec在每个 Pod 中执行hostname。 for i in 0 1; do kubectl exec \"web-$i\" -- sh -c 'hostname'; done web-0 web-1 web-2 使用 kubectl run 运行一个提供 nslookup 命令的容器，该命令来自于 dnsutils 包。 通过对 Pod 的主机名执行 nslookup，你可以检查他们在集群内部的 DNS 地址。 kubectl run -i --tty --image busybox:1.28 dns-test --restart=Never --rm 这将启动一个新的 shell。在新 shell 中，运行： # Run this in the dns-test container shell nslookup statefulset-service 输出类似于： Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: statefulset-service Address 1: 10.244.169.165 web-0.statefulset-service.default.svc.cluster.local Address 2: 10.244.36.98 web-1.statefulset-service.default.svc.cluster.local Address 3: 10.244.169.166 web-2.statefulset-service.default.svc.cluster.local 即使 Pod 重建之后 IP 发生改变，Headless Service 还是能够根据 web-{0-1}.statefulset-service.default.svc.cluster.local 这个 DNS A 记录来找到每个 pod 的 IP 地址。 标准 Service 和 Headless Service 的区别 这里要提到 无状态应用控制器 和 有状态应用控制器 的设计理念，无状态的 Pod 是完全相等的，提供相同的服务，可以飘移在任意节点，例如三个 NGINX Pod 所提供的 Web 服务。而像一些分布式应用程序，例如 zookeeper 集群、etcd 集群、mysql 主从等服务，每个实例都会维护着一种状态，每个实例都有自己的数据，并且每个实例之间必须有固定的访问地址（组建集群），这就是有状态应用。由于标准 Service 是通过访问 ClusterIP 负载均衡到一组 Pod 上，这是没有办法指定访问到某个 Pod 的（由 iptables 决定）。所以这里就出现了 Headless Service ，而且 Headless Service 不需要 ClusterIP ，它是通过访问 Pod DNS 名称解析到对应的 Pod IP，为每一个 Pod 都固定一个 DNS 名称，即使 Pod 的 IP 发生改变，Pod 的 DNS 名称还是指向对应的 Pod IP 地址。 写入稳定的存储 Kubernetes 为每个 VolumeClaimTemplate 创建一个 PersistentVolume。 在上面的 nginx 示例中，每个 Pod 将会得到基于 StorageClass managed-nfs-storage 提供的 2 Gib 的 PersistentVolume。如果没有声明 StorageClass，就会使用默认的 StorageClass。 当一个 Pod 被调度（重新调度）到节点上时，它的 volumeMounts 会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume。 请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。 获取 StatefulSet 创建的 PersistentVolumeClaims。 kubectl get pvc -l app=nginx 输出类似于： NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-83489225-7d08-4506-9f75-1fefd3aee287 2Gi RWO managed-nfs-storage 23h www-web-1 Bound pvc-e5afdbfa-952e-4120-8885-00cf5a524eb0 2Gi RWO managed-nfs-storage 23h www-web-2 Bound pvc-21e9a5b1-a040-41c6-9b94-6bcf7a8a8966 2Gi RWO managed-nfs-storage 23h StatefulSet 控制器创建了三个 PersistentVolumeClaims，绑定到三个 PersistentVolumes。由于本教程使用的集群配置为动态提供 PersistentVolume，所有的 PersistentVolume 都是自动创建和绑定的，对于动态配置的 PersistentVolumes 来说，默认回收策略为 \"Delete\"。 NGINX web 服务器默认会加载位于 /usr/share/nginx/html/index.html 的 index 文件。 StatefulSets spec 中的 volumeMounts 字段保证了 /usr/share/nginx/html 文件夹由一个 PersistentVolume 支持。 将 Pod 的主机名写入它们的index.html文件并验证 NGINX web 服务器使用该主机名提供服务。 for i in 0 1 2; do kubectl exec \"web-$i\" -- sh -c 'echo $(hostname) > /usr/share/nginx/html/index.html'; done for i in 0 1 2; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 在另一个终端删除 StatefulSet 所有的 Pod。 kubectl delete pod -l app=nginx 验证所有 web 服务器在继续使用它们的主机名提供服务。 for i in 0 1 2; do kubectl exec -i -t \"web-$i\" -- curl http://localhost/; done 虽然 Podweb-{0-2} 被重新调度了，但它们仍然继续监听各自的主机名，因为和它们的 PersistentVolumeClaim 相关联的 PersistentVolume 被重新挂载到了各自的 volumeMount 上。 不管 Pod 被调度到了哪个节点上，它们的 PersistentVolumes 将会被挂载到合适的挂载点上。 "},"worknote/kubernetes/certification-authorization-access-control.html":{"url":"worknote/kubernetes/certification-authorization-access-control.html","title":"认证授权与准入控制","keywords":"","body":"认证、授权与准入控制 在任何将资源或服务提供给有限使用者的系统上，认证和授权是两个必不可少的功能，前者用于身份鉴别，负责验证“来者是谁”，而后者则实现权限分派，负责验证“他有权做什么事”。Kubernetes 系统完全分离了身份验证和授权功能，将二者分别以多种不同的插件实现，而且还有特有的准入控制机制，能在“写”请求上辅助完成更为精细的操作验证及变异功能。 Kubernetes 访问控制 API Server 作为 Kubernetes 集群系统的网关，是访问及管理资源对象的唯一入口，它默认监听 TCP 的 6443 端口，通过 HTTPS 协议暴露了一个 RESTful 风格的接口。所有需要访问集群资源的集群组件或客户端，包括 kube-controller-manager、kube-scheduler、kubelet 和 kube-proxy 等集群基础组件，CoreDNS 等集群附加组件，以及 kubectl 命令等都必须要经过网关请求与集群通信。所有客户端均要经由 API Server 访问或改变集群状态以及完成数据存储，并且 API Server 会对每一次的访问请求进行合法检验，包括用户身份鉴别，操作权限验证以及操作是否符合全局规范的约束等。所有检查均正常完成且对象配置信息合法性检验无误后才能访问或存入数据到后端存储系统 ETCD 中。 客户端认证操作由 API Server 配置的一到多个认证插件完成。收到请求后，API Server 依次调用配置的认证插件来校验客户端的身份，直到其中一个插件可以识别出请求者的身份为止。授权操作则由一到多个授权插件完成，这些插件负责确定通过认证的用户是否有权限执行发出的资源请求，该类操作包括创建、读取、删除或修改指定的对象等。随后通过授权检测的用户请求修改相关的操作还要经由一到多个准入控制插件的遍历式检测，例如使用默认值补足要创建的目标资源对象中未定义的各个字段、检查目标 Namespace 资源对象是否存在、检查请求创建的 Pod 对象是否违反系统资源限制等，其中的任何检查失败都可能导致写入操作失败。 用户账号与用户组 Kubernetes 系统上的用户账号及用户组的实现机制与常规应用略有不同。Kubernetes 集群将那些通过命令行工具 kubectl 、客户端库或者直接使用 RESTful 接口向 API Server 发起请求的客户端上的请求主体分为两个不同的类别：现实中的“人”和 Pod 对象，它们的用户身份分别对应用户账号（User Account，也称普通用户）和服务账号（Service Account，简称 SA）。 用户账户：其使用主体往往是“人”，一般由外部的用户管理系统存储和管理，Kubernetes 本身不维护这一类的任何用户账户信息，他们不会存储到 API Server 之上，仅仅用于检验用户是否有权限执行其所请求的操作。 服务账号：其使用主体是“应用程序”，专用于为 Pod 资源中的服务进程提供访问 Kubernetes API 时的身份标识（identity），Service Account 资源通常要绑定到特定的名称空间，它们由 API Server 自动创建或通过 API 调用，由管理员手动创建，通常附带着一组访问 API Server 的认证凭据 ------ Secret，可由同一名称空间的 Pod 应用访问 API Server 时使用。 用户账号通常是用于复杂的业务逻辑管控，作用于系统全局，因而名称必须全局唯一。Kubernetes 并不会存储由认证插件从客户端请求中提取的用户及所属的组信息，因而也就没有办法对普通用户进行身份认证，他们仅仅用于检验该操作主体是否有权限执行其所请求的操作。相比较来说，服务账号则隶属于名称空间级别，仅用于实现某些特定操作任务，因此功能上要轻量得多。这两类账号都可以隶属于一个或多个用户组。 对 API Server 来说，来自客户端的请求要么与用户账户进行绑定，要么以某个服务账户的身份进行，否则会被视为匿名请求。这意味着集群内部或外部的每个进程，包括由人类用户使用 kubectl，以及各节点上运行的 kubelet 进程，再到控制平面的成员组件，必须在向 API Server 发出请求时进行身份验证。 认证、授权、准入控制基础 Kubernetes 使用身份验证插件对 API Server 请求进行身份验证，它允许管理员自定义服务账号和用户账号要启用或禁用的插件，并支持各自同时启用多种认证机制。具体设定时，至少应该为服务账号和用户账号各自启用一个认证插件。 如果启用了多种认证机制，账号认证过程由认证插件以串行的方式进行，直到其中一种认证机制成功完成即结束。若认证失败，服务器则返回 401 状态码，反之，请求者就会被 Kubernetes 识别为某个具体的用户（以其用户名进行标识），并且该连接上随后的操作都会以此用户身份进行。API Server 对于接收到的每个访问请求会调用认证插件，尝试将以下属性与访问请求相关联。 用户名：用户名，例如 Kubernetes-admin 等。 用户 ID: 用户的数字标签符，用于确保用户身份的唯一性。 用户组：用户所属的组，用于权限指派和继承， 常见的值可能是 system:masters 或者 devops-team 等。 附加字段：键值数据类型的字符串，用于提供认证需要用到的额外信息。 API Server 支持以下几种具体的认证方式，其中所有的令牌认证机制通常被统称为“承载令牌认证”。 X509 客户证书认证：通过给 API 服务器传递 --client-ca-file=SOMEFILE 选项，就可以启动客户端证书身份认证。 所引用的文件必须包含一个或者多个证书机构，用来验证向 API 服务器提供的客户端证书。 如果提供了客户端证书并且证书被验证通过，则 subject 中的公共名称（Common Name）就被作为请求的用户名。 静态令牌文件认证：当 API 服务器的命令行设置了 --token-auth-file=SOMEFILE 选项时，会从文件中读取持有者令牌。目前，令牌会长期有效，并且在不重启 API 服务器的情况下无法更改令牌列表。 启动引导令牌认证：一种动态管理承载令牌进行身份认证的方式，常用于简化组建新 Kubernetes 集群时将节点加入集群的认证过程，需要由 Kube-apiserver 通过 --enable-bootstrap-token-auth 选项启用，新的工作节点首次加入时，Master 使用引导令牌确认节点身份的合法性之后自动为其签署数字证书以用于后续的安全通信，kubeadm 初始化的集群也是这种认证方式。 Server Account 令牌：该认证方式会由 kube-apiserver 程序自动启用，它同样使用签名的承载令牌来验证请求，该认证方式还支持通过可选项 --service-account-key-file 加载签署承载令牌的秘钥文件，未指定时将使用 API Server 自己的 TLS 私钥，Server Account 通常由 API Server 自动创建，并通过 Server Account 准入控制器将其注入 Pod 对象，包括 Server Account 上的承载令牌，容器中的应用程序请求 API Server 的服务时以此完成身份认证。 那些未能被任何验证插件明确拒绝的请求中的用户即为匿名用户，该类用户会被冠以 system：anonymous 用户名，隶属于 system: unauthenticated 用户组。若 API Server 启用了除 Always Allow 以外的认证机制，则匿名用户处于启用状态，但是，处于安全因素考虑，建议管理员通过 --anonymous-auth=false 选项将其禁用。 除了身份信息，请求报文还需要提供操作方法及其目标对象，例如针对某 Pod 资源对象进行的创建、查看、修改或者删除操作等。具体包含以下信息。 API: 用于定义请求的目标是否为一个 API 资源。 Request path: 请求的非资源路径，例如 /api 或 /healthz。 API group: 要访问的 API 组，仅对资源型请求有效，默认为 core API group。 Namespace: 目标资源的名称空间，仅对于隶属于名称空间类型的资源有效。 API request verb: API 请求类的操作，即资源请求，包括 get、list、create、update、patch、watch、delete 等。 HTTP request verb: HTTP 请求类的操作，即非资源类请求要执行的操作，如 get、post、put、delete 等。 Resource: 请求的目标资源的 ID 或名称。 Subersource: 请求的子资源。 为了核验用户的操作许可，成功通过身份认证后的操作请求还需要转交给授权插件进行许可权限检查，以确保其拥有相应操作的许可。API Server 只要支持使用 4 类内置的授权插件来定义用户的操作权限。 Node: 基于 Pod 资源的目标调度节点来实现对 kubelet 的访问控制。 ABAC: Attribute-based access control，基于属性的访问控制。 RBAC: Role-based access control，基于角色的访问控制。 Webhook: 基于 HTTP 回调机制实现外部 REST 服务检查，确认用户授权的访问控制。 另外，还有 AlwaysDeny 和 AlwaysAllow 两个特殊的授权插件，其中 AlwaysDeny（总是拒绝）仅用于测试，而 AlwaysAllow（总是允许），则用于不期望进行授权检查时直接在授权检查阶段放行所有的操作请求。--authorization-mode 选项用于定义 API Server 要启用的授权机制，多个选项值彼此间以逗号进行分隔。 而准入控制器则用于在客户端请求经过身份验证和授权检查之后，将对象持久化存储到 etcd 之前拦截请求，从而实现在资源的创建，更新和删除操作期间强制执行对象的语义验证等功能，而读取资源信息的操作请求则不会经由准入控制器检查。API Server 内置了许多准入控制器，常用的包含下面列出的几种。 AlwaysAdmin 和 AlwaysDeny: 前者允许所有请求，后者则拒绝所有请求。（已废弃，仅了解） AlwaysPullmages: 总是下载镜像，即每次创建 Pod 对象之前都要去下载镜像。 NamespaceLifecycle: 拒绝在不存在的名称空间中创建资源，而删除名称空间则会级联删除其下的所有其他资源。 LimitRanger: 可用资源范围界定，用于对设置了 LimitRange 的对象所发出的所有请求进行监控，以确保其资源请求不会超限。 ServiceAccount: 用于实现服务账号管控机制的自动化，实现创建 Pod 对象时自动为其附加相关的 Service Account 对象。 DefaultStorageClass: 监控所有创建 PVC 对象的请求，以保证那些没有附加任何专用 StorageClass 的请求会被自动设定一个默认值。 ResourceQuota: 用于为名称空间设置可用资源上限，并确保当其中创建的任何设置了资源限额的对象时，不会超出名称空间的资源配额。 早期的准入控制器代码需要由管理员编译进 kube-apiserver 中才能使用，实现方式缺乏灵活性。于是 Kubernetes 自 v1.7 版本引入了 Initializers 和 External Admin Webhooks 来尝试突破此限制，而且 v1.9 版本起，External Admin Webhooks 被分为 Mutating-Admission Webhooks 和 ValidatingAdmission Webhooks 两种类型，分别用于在 API 中执行对象配置的变异和验证操作。检查期间，仅那些顺利通过所有准入控制器检查的资源操作请求的结果才能保存到 etcd 中，而任何一个准入控制器的拒绝都将导致写入请求失败。 ServiceAccount 及认证 Kubernetes 原生的应用程序意味着专为运行于 Kubernetes 系统之上而开发的应用程序，这些程序托管运行在 Kubernetes 之上，能够直接与 API Server 进行交互，并进行资源状态的查询或更新，例如 Flannel 和 CoreDNS 等。显然，API Server 同样需要对来自 Pod 资源中的客户端程序进行身份验证，服务账号也是专用于这类场景的账号。ServiceAccount 资源一般由用户身份信息及保存了认证信息的 Secret 对象组成。 ServiceAccount 自动化 我们创建的每个 Pod 资源都自动关联了一个 Secret 存储卷，并由其容器挂载至 /var/run/secret/kubernetes.io/serviceaccount 目录。各容器的挂载点目录通常存在 3 个文件：ca.crt、namespace 和 token，其中，token 文件保存了 ServiceAccount 的认证令牌，容器中的进程使用该账户认证到 API Server ，进而由认证插件完成用户认证并将其用户名传递给授权插件。 每个 Pod 对象只有一个服务账号，若创建 Pod 资源时未予以明确指定，则 ServiceAccount 准入控制器会为其自动附加当前名称空间中默认的服务账号，其名称通常为 default。 Kubernetes 系统通过 3 个独立的组件相互协作实现了上面描述的 Pod 对象服务账号的自动化过程：ServiceAccount 准入控制器、令牌控制器和 ServiceAccount 控制器。ServiceAccount 控制器负责为名称空间管理相应的资源对象，它需要确保每个名称空间中都存在一个名为 default 的服务账号对象。ServiceAccount 准入控制器内置在 API Server 中，负责在创建或更新 Pod 时按需进行 ServiceAccount 资源对象相关信息的修改，这包括如下操作。 若 Pod 没有显式定义使用的 ServiceAccount 对象，则将其设置为 default。 若 Pod 显式引用了 ServiceAccount，则负责检查被引用的对象是否存在，不存在时将拒绝 Pod 资源的创建请求。 若 Pod 中不包含 ImagePullSecret，则把 ServiceAccount 的 ImagePullSecret 附加其上。 为带有访问 API 的令牌的 Pod 对象添加一个存储卷。 为 Pod 对象中的每个容器添加一个 volumeMount，将 ServiceAccount 的存储卷挂载至 /var/run/secret/kubernetes.io/serviceaccount。 令牌控制器是控制平面组件 Controller Manager 中的一个专用控制器，它工作于异步模式，负责完成如下任务。 监控 ServiceAccount 的创建操作，并为其添加用于访问 API 的 Secret 对象。 监控 ServiceAccount 的删除操作，并删除其相关的所有 ServiceAccount 令牌秘钥。 监控 Secret 对象的添加操作，确保其引用的 ServiceAccount 存在，并在必要时为 Secret 对象添加认证令牌。 监控 Secret 对象的删除操作，以确保删除每个 ServiceAccount 对此 Secret 的引用。 ServiceAccount 基础应用 ServiceAccount 是 Kubernetes API 上的一种资源类型，它属于名称空间级别，用于让 Pod 对象内部的应用程序在与 API Server 通信时完成身份认证。 命令式 ServiceAccount 资源创建： kubectl create serviceaccount 命令能够快速创建自定义的 ServiceAccount 资源，我们仅需要在命令后给出目标 ServiceAccount 资源的名称。 [root@k8s-master ~]# kubectl create serviceaccount my-service-account serviceaccount/my-service-account created Kubernetes 会为创建的 ServiceAccount 资源自动生成并附加一个 Secret 对象，该对象以 ServiceAccount 资源名称为前缀。该 Secret 对象属于特殊的 kubernetes.io/service-account-token 类型，它包含 ca.crt、namespace 和 secret 这 3 个数据项，它们分别是 Kubernetes Root CA 证书、Secret 对象所在名称空间和访问 API Server 的令牌。 ServiceAccount 资源清单： 更完善的创建 ServiceAccount 资源的方式是使用资源规范，该规范比较简单，它没有 spec 字段，仅指定了资源名称，以及允许 Pod 对象将其自动挂载为存储卷，引用的 Secret 对象则由系统自动生成。 apiVersion: v1 kind: ServiceAccout metadata: name: sa-demo namespace: default automountServiceAccountToken: true # 是否让Pod自动挂载API令牌 kubeconfig 配置文件 基于无状态协议 HTTP/HTTPS 的 API Server 需要验证每次连接请求中的用户身份，因而 kube-controller-manager、kube-scheduler 和 kube-proxy 等各类客户端组件必须能自动完成身份认证信息的提交，但通过程序选项来提供这些信息会导致敏感信息泄露。另外，管理员还面临着使用 kubectl 工具接入不同集群时的认证及认证信息映射难题。为此，Kubernetes 设计了一种称为 kubeconfig 的配置文件，它保存有接入一到多个 Kubernetes 集群的相关配置信息，并允许管理员按需在各配置间灵活切换。 kubernetes cluster1 API Server kubectl ---> kubeconfig ---> kubernetes cluster2 API Server kubernetes cluster3 API Server 客户端程序可以通过默认路径、--kubeconfig 选项或者 KUBECONFIG 环境变量自定义要加载的 kubeconfig 文件，从而能够在每次的访问请求中可认证到目标 API Server。 kubeconfig 文件格式 kubeconfig 文件中，各集群的接入端点以列表形式定义在 clusters 配置段中，每个列表项代表一个 Kubernetes 集群，并拥有名称识别；各身份认证信息定义在 users 配置段中，每个列表项代表一个能够认证到某 Kubernetes 集群的凭据。将身份凭据与集群分开定义以便复用，具体使用时还要以 context（上下文）在二者之间按需建立映射关系，各 context 以列表形式定义在 context 配置段中，而当前使用的映射关系则定义在 current-context 配置段中。 clusters: - cluster: name: kubernetes ...... users: - name: kubernetes-admin ...... contexts: - context: name: kubernetes-admin@kubernetes ...... current-context: kubernetes-admin@kubernetes 使用 kubeadm 初始化 Kubernetes 集群过程中，在 Master 节点上生成的 /etc/kubernetes/admin.conf 文件就是一个 kubeconfig 格式的文件，它由 kubeadm init 命令自动生成，可由 kubectl 加载后接入当前集群的 API Server。kubeconfig 文件的默认加载路径为 $HOME/.kube/config，在 kubeadm init 命令初始化集群过程中有一个步骤便是将 /etc/kubenetes/admin.conf 复制为该默认搜索路径上的文件。当然也可以通过 --kubeconfig 选项或 KUBECONFIG 环境变量将其修改为其他路径。 kubectl config view 命令能打印 kubeconfig 文件的内容，下面的命令结果显示了默认路径下的文件配置，包括集群列表、用户列表、上下文列表以及当前使用的上下文等。 [root@k8s-master ~]# kubectl config view apiVersion: v1 kind: Config preferences: {} clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://10.10.110.190:6443 name: kubernetes users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes 用户可以在 kubeconfig 配置文件中按需自定义相关的配置信息，以实现使用不同的用户账号接入集群等功能。kubeconfig 是一个文本文件，尽管可以使用文本处理工具直接编辑，但强烈建议用户使用 kubectl config 及其子命令进行该文件的设定，以便利用其它自动进行语法检测等额外功能。kubectl config 的常用子命令有如下几项。 view: 打印 kubeconfig 文件内容。 set-cluster: 设定新的集群信息，以单独的列表项保存于 cluster 配置段。 set-credentials: 设置认证凭据，保存为 users 配置段的一个列表项。 set-context: 设置新的上下文信息，保存为 context 配置段的一个列表项。 use-context: 设定 current-context 配置段，确定当前以哪个用户的身份接入到哪个集群当中。 delete-cluster: 删除 cluster 中指定的列表项。 delete-context: 删除 context 中指定的列表项。 get-cluster: 获取 cluster 中定义的集群列表。 get-context: 获取 context 中定义的上下文列表。 自定义 kubeconfig 文件 通常，一个完整 kubeconfig 配置文件的定义至少应该包括集群、身份凭证、上下文以及当前上下文 4 项，但在保存有集群身份和身份凭据的现有 kubeconfig 文件基础上添加新的上下文时，可能只需要提供身份凭据而复用现有的集群定义，具体操作步骤需要按实际情况判定。 示例：为 dev 用户授权 default 命名空间下 Pod 读取权限。 以证书认证方式授权 kubeconfig（使用现有 Kubernetes CA 签发 dev 证书）。 cat > ca-config.json kubernetes-dev-csr.json 设置集群参数，包括集群名称、API Server URL 和 kubenetes-ca 证书。 kubectl config set-cluster dev --embed-certs=true \\ --certificate-authority=/etc/kubernetes/pki/ca.crt \\ --server=\"https://10.10.110.190:6443\" \\ --kubeconfig=$HOME/.kube/kube-dev.cnofig 添加身份凭据，使用前面生成的 dev 证书认证客户端。 kubectl config set-credentials kubernetes-dev \\ --client-key=kubernetes-dev-key.pem \\ --client-certificate=kubernetes-dev.pem \\ --embed-certs=true \\ --kubeconfig=$HOME/.kube/kube-dev.cnofig 以用户 kubernetes-dev 的身份凭据与 dev 集群建立映射关系。 kubectl config set-context kubernetes-dev@dev \\ --cluster=dev \\ --user=kubernetes-dev \\ --kubeconfig=$HOME/.kube/kube-dev.cnofig 设置 kube-dev.cnofig 配置文件当前上下文为 kubernetes-dev@dev。 kubectl config use-context kubernetes-dev@dev --kubeconfig=$HOME/.kube/kube-dev.cnofig 使用 kube-dev.cnofig 配置文件进行访问测试。 kubectl get namespaces --kubeconfig=$HOME/.kube/kube-dev.cnofig Error from server (Forbidden): namespaces is forbidden: User \"kubernetes-dev\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope 这是因为 kubernetes-dev 用户没有绑定 Role，所以不具有任何权限。 基于角色的访问控制：RBAC DAC（自主访问控制）、MAC（强制访问控制）、RBAC（基于角色的访问控制）和 ABAC（基于属性的访问控制）这四种主流的权限管理模型中，Kubernetes 支持使用后两种完成普通账户和服务账户的权限管理，另外支持的权限管理模型还有 Node 和 Webhook 两种。 RBAC 是一种新型、灵活且使用广泛的访问控制机制，它将权限授予角色，通过让“用户”扮演一到多个“角色”完成灵活的权限管理，这有别于传统访问控制机制中将权限直接赋予使用者的方式。相对于 Kubernetes 支持的 ABAC 和 Webhook 等授权机制，RBAC 具有如下优势： 对集群中的资源和非资源型 URL 的权限实现了完整覆盖。 整个 RBAC 完全由少数几个 API 对象实现，而且与其他 API 对象一样可以使用 kubectl 或 API 调用进行操作。 支持权限的运行时调整，无须重新启动 API Server。 RBAC 授权模型 RBAC 是一种特定的权限管理模型，它把可以施加在“资源对象”上的“动作”称为“许可权限”，这些许可权限能够按需组合在一起构建出“角色”及其职能，并通过为“用户账户或组账户”分配一到多个角色完成权限委派。这些能够发出动作的用户在 RBAC 中也称为“主体”。 Role1 user ---> Role2 ---> 权限（操作--->对象） Role3 RBAC 访问控制模型中，授权操作只能通过角色完成，主体只有在分配到角色之后才能行使权限，且仅限于从其绑定的各角色之上继承而来的权限。换句话说，用户的权限仅能够通过角色分配获得，未能得到显示角色委派的用户则不具有任何权限。 简单来说，RBAC 就是一种访问控制模型，它以角色为中心界定“谁”（subject）能够“操作”（verb）哪个或哪类“对象”（object）。动作的发出者即“主体”，通常以“账户”为载体，在 Kubernetes 系统上，它可以是普通账户，也可以是服务账户。“动作”用于表明要执行的具体操作，包括创建、删除、修改和查看等行为，对于 API Server 来说，即 PUT、DELETE 和 GET 等请求方法。而“对象”则是指管理操作能够施加的目标实体，对 Kubernetes API 来说主要指各类资源对象以及非资源型 URL。 Kubernetes 系统的 RBAC 授权插件将角色分为 Role 和 ClusterRole 两类，它们都是 Kubernetes 内置支持的 API 资源类型，其中 Role 作用于名称空间级别，用于承载名称空间内的资源权限集合，而 ClusterRole 则能够同时承载名称空间和集群级别的资源权限集合。Role 无法承载集群级别的资源类型的操作权限，这类的资源包括集群级别的资源（例如 Nodes），非资源类型的端点（例如 /healthz），以及作用于所有名称空间的资源等。 利用 Role 和 ClusterRole 两类角色进行赋权时，需要用到另外两种资源 RoleBinding 和 ClusterRoleBinding，它们同样时由 API Server 内置支持的资源类型。RoleBinding 用于将 Role 绑定到一个或者一组用户之上，它隶属于且仅能作用于其所在的单个名称空间。RoleBinding 可以引用同一名称空间中的 Role，也可以引用集群级别的 ClusterRole，但引用 ClusterRole 的许可权限会降低到仅能在 RoleBinding 所在的名称空间生效。而 ClusterRoleBinding 则用于将 ClusterRole 绑定到用户或组，它作用于集群全局，且仅能够引用 ClusterRole。 Role 和 ClusterRole 如前所述，Role 和 ClusterRole 是 API Server 内置的两种资源类型，它们在本质上都只是一组许可权限的集合。 下面的配置清单示例在 default 名称空间中定义了一个 Role 的资源，它设定了读取、列出及监视 pods 和 services 资源。 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pods-reader rules: - apiGroup: [\"\"] # \"\" 表示核心 API 群组 resources: [\"pods\", \"services\"] verbs: [\"get\", \"list\", \"watch\"] ClusterRole 资源隶属于集群级别，它引用名称空间级别的资源意味着相关的操作权限能够在所有名称空间生效，同时，它也能够引用 Role 所不支持的集群级别的资源类型，例如 nodes 和 persistentvolumes 等。下面的示例清单定义了 ClusterRole 资源，它拥有管理集群节点信息的权限。ClusterRole 不属于名称空间，所以其配置不能够使用 metadata.namespace 字段。 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: nodes-admin rules: - apiGroup: [\"\"] # \"\" 表示核心 API 群组 resources: [\"nodes\"] verbs: [\"*\"] Role 或 ClusterRole 对象本身并不能作为动作的执行主体，它们需要“绑定”到主体（例如 User、Group 或 Service Account）之上完成赋权，而后由相应主体执行资源操作。 RoleBinding 与 ClusterRoleBinding "},"worknote/docker/readme.html":{"url":"worknote/docker/readme.html","title":"Docker","keywords":"","body":"Docker 1.概述 2.安装 3.镜像和容器 4.数据持久化 5.网络 6.Dockerfile "},"worknote/docker/overview.html":{"url":"worknote/docker/overview.html","title":"概述","keywords":"","body":"概述 Docker 是什么？ Docker 翻译过来就是“码头工人”，而它搬运的东西就是集装箱（Container），Container 里面存放的是不同类型的程序和代码。Docker 能够将应用程序和基础设施分开，以便我们实现程序在各个主流系统之间进行快速交付、测试和部署。以 Image 的方式交付程序可以实现标准化、可移植的优点，减少开发和生产中运行代码的差异。 Docker 属于容器技术的一种，但近几年来 Docker 已经火得成为了容器的代名词。早在 1979 年，Chroot jail 被用于“Change Root”，它被认为是最早的容器化技术之一，后来 FreeBSD Jail 于 2000 年在 FreeBSD OS 中引入，旨在为简单的 Chroot 文件隔离带来更多安全性。2013 年，Docker 推出了第一个版本，依赖 Linux 内核特性 namespace 和 Cgroup，实现操作系统级别虚拟化。 可以使用 Docker 做什么？ 快速、一致的交付应用程序 开发人员使用容器可以使程序和服务在标准化环境中工作，从而缩短了开发生命周期。容器非常适合持续集成和持续部署（CI/CD）工作流。 工作中容器有以下场景： 开发人员在本地编写代码可以使用 Docker 容器与同事共享。 使用 Docker 将他们的程序推送到测试环境并执行自动化和测试。 发现 Bug 时，在开发环境进行修复，并重新部署到测试环境进行测试和验证。 测试完成后，为客户提供修复就把更新的镜像推送到生产环境。 响应式部署和拓展 Docker 容器可移植性非常高，可以运行在开发人员的本地电脑、虚拟机、数据中心的物理服务器、各种云提供商。 Docker 的轻量级特性可以让我们轻松的动态管理应用程序，根据业务的需求可以实时的拓展和删除。 相同硬件条件下运行更多应用程序 Docker 非常轻量和快速，操作系统级别的虚拟化，可以使 Docker 运行更多应用程序，Docker 非常适合高密度环境以及需要以更少资源完成更多任务的中小型部署。 Docker Engine Docker Engine 是用来运行和管理容器的核心程序，主要包含以下组件： 后台运行的守护进程 dockerd。 和 dockerd 进行交互的 REST API Server。 命令行 CLI 接口，和 REST API Server 进行交互（就是常用的 docker 命令）。 Docker 架构 Docker 使用 C/S （客户端/服务端）架构。Docker 客户端与 Docker 守护进程通信，后者负责构建、运行、分发 Docker 容器。Docker 客户端和守护程序可以运行在同一系统上，你也可以将 Docker 客户端连接到远程 Docker 守护进程。Docker 客户端和守护进程使用 REST API 通过 UNIX 套接字或网络接口进行通信。另一个 Docker 客户端是 Docker Compose，它是使用一组容器组成的应用程序。 Docker Damon: dockerd，负责监听 Docker API 的请求和管理 Docker 对象，例如镜像、容器、网络和 volume。 Docker Client: docker，Docker Client 是用户和 Docker 交互的主要方式，诸如使用 docker run 此类命令时，client 会将这些命令发送到 dockerd 由 dockerd 去执行。 Image: 镜像，镜像是带有创建 Docker 容器命令说明的只读模板，通常，镜像基于另外的一些基础镜像并加上一些自定义的功能，例如我构建一个基于 Ubuntu 的镜像，并在这个镜像安装 Nginx，这样就构建了一个属于我们的镜像。 Registry: 镜像仓库，存储着我们构建的镜像。镜像仓库分为公有仓库和私有仓库，Docker Hub 是一个由 Docker 公司运行和管理的基于云的公有镜像仓库，Docker Hub 允许任何用户自由的发布和使用镜像，对于公司或者组织来说，通常使用的是内部的私有仓库，私有仓库的镜像仅允许内部人员使用。 Containers: 容器，容器是一个镜像的可运行实例，可以使用 Docker REST API 或者 CLI 来操作容器，容器的实质是系统上的一个进程，但与直接在宿主执行的进程不同，容器进程运行在属于自己的独立命名空间。因此容器可以拥有独立的文件系统、网络配置、进程空间，用户空间。 底层技术支持：Namespaces（隔离）、CGroups（资源限制）、UnionFS（联合文件系统）、COW（写时复制）。 Docker 与虚拟机 在传统虚拟化中，Hypervisor 基于物理硬件进行虚拟化，每一个 VM 都包含一个操作系统、操作系统运行时所需的硬件虚拟文件、应用程序及其关联的库文件和依赖项。不同操作系统的虚拟机可以运行在同一台物理服务器上，每个虚拟机之间是操作系统级别的隔离。 容器不是基于底层物理硬件进行的虚拟化，而是基于操作系统的虚拟化，因此每个单独的容器只包含应用程序及其关联的库文件和依赖项，这也是容器轻量快速的原因。因为所有容器都共享宿主机的内核和物理资源，并不是每个容器都包含一个操作系统，所有容器之间是进程级别的隔离。 虚拟机基于物理硬件进行虚拟化，容器基于操作系统的虚拟化，这是虚拟机和容器最大的区别。容器虽然更加轻量快速，但是因为共享一个底层内核，因此容器与虚拟机在资源隔离方面有着先天的劣势，这也是目前容器技术暂时不会取代虚拟机模式的原因之一。虚拟机的每一个实例都是一个完整的操作系统，当虚拟机数量增加时会非常耗费系统的资源，这也是虚拟机的劣势。虚拟机和容器一样，都可以通过提高物理机的 CPU 和内存来获得创建更多实例的机会。然而，容器在未来会走得更远，因为它支持微服务架构，可以更精细的部署和拓展应用程序组件。 "},"worknote/docker/install.html":{"url":"worknote/docker/install.html","title":"安装","keywords":"","body":"安装 Docker 引擎支持 Mac 和 Windows桌面版，还有支持各种 Linux 的主流发行版本，Docker 官方安装文档参考。 Docker Engine 有 3 种更新通道：stable、test、nightly。 stable: 最新并且可用的稳定版本。 test: GA 发布之前的预览测试版本。 nightly: 针对下一个主要发行版本的每晚自动构建包。 CentOS 安装 Docker 操作系统要求 要安装 Docker 引擎，您需要 CentOS 7 或 8 的维护版本，不支持测试或存档版本。 必须启用 centos-extras 库，默认情况下已启用此存储库，但如果你已禁用它，则需要重新启用它。 推荐使用 overlay2 存储驱动。 卸载旧版本 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 安装方法 可以根据以下不同的方式去安装 Docker: 大部分用户使用设置 Docker 存储库的方法去安装，这也是官方推荐的安装方法。 下载 RPM 包进行手动安装和管理，这在没有外网的机器上安装 Docker 非常有用。 在测试和开发环境中，使用脚本自动化安装 Docker。 使用存储库安装 # 设置存储库 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 Docker 引擎 sudo yum install -y docker-ce docker-ce-cli containerd.io Ubuntu 安装 Docker 操作系统要求 要安装 Docker，你需要以下 Ubuntu 64 位版本： Ubuntu 20.04 (LTS) Ubuntu 18.04 (LTS) 卸载旧版本 sudo apt-get remove docker docker-engine docker.io containerd runc 使用存储库安装 # 更新 apt 包索引并安装包 sudo apt-get update sudo apt-get install -y \\ ca-certificates \\ curl \\ gnupg \\ lsb-release # 添加 Docker 官方的 GPG 密钥 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg # 设置稳定存储库 echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null # 安装 Docker 引擎 sudo apt-get update sudo apt-get install -y docker-ce docker-ce-cli containerd.io "},"worknote/docker/images-and-containers.html":{"url":"worknote/docker/images-and-containers.html","title":"镜像和容器","keywords":"","body":"镜像和容器 镜像依赖技术 镜像相当于是一个 root 文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含一些为运行时准备的配置参数：例如匿名卷、环境变量、用户等。镜像不包含任何动态的数据，镜像内容在其构建之后就不会被改变。一个镜像可以被重复使用多次，创建无数个相同的容器，所以我们在前面说镜像是带有创建 Docker 容器命令说明的只读模板。镜像还可以是一种标准化的软件交付方式，镜像内包含基础运行环境和程序代码。 镜像的分层依赖于一系列的底层技术，例如联合文件系统（Union FileSystem）、写时复制（copy no write）。 联合文件系统（Union FileSystem） 联合文件系统可以将多个目录（也称为分支，因为联合文件系统支持对文件系统的修改作为一次提交来层层叠加）的内容联合挂载到一个目录下，而不修改目录的物理位置。联合文件系统还支持只读和可读写目录共存，这是实现镜像拥有只读层和可读写层的原理。联合文件系统是 Docker 镜像的基石，镜像的分层可以让不同容器之间共享资源。例如两个使用 Ubuntu:20.04 镜像创建的容器，一个容器安装了 Nginx，另一个容器安装了 Tomcat，那么这两个容器其实是共享 Ubuntu:20.04 镜像层，只有安装 Nginx 和 Tomcat 的那一层才是它们自己的改动层，联合文件系统在实现资源共享的同时提高了存储效率。 Docker 目前支持的联合文件系统有：overlay2、overlay、aufs、btrfs、vfs、devicemapper。 aufs 是 Docker 以前版本默认的存储驱动，如果 Linux 内核是 4.0 或者更高版本，并且使用 Docker Engine - Community，请使用新的 overlay2，它的性能比 aufs 存储驱动更好。 写时复制（copy no write） 我们已经知道 Docker 镜像是由多个只读层叠加构建的，那么在启动容器时，我们对容器的操作是如何被保存的呢？其实，Docker 启动时会加载镜像的只读层并在镜像层的顶部添加一个读写层，如果此时在容器中修改了一个已存在的文件，该文件将会从读写层下面的只读层复制到读写层并进行修改，该文件的只读版本仍然存在，但是容器已经无法看到存在于下层的这个文件了。只有被修改的文件才会被复制到读写层，写时复制最大限度减少了 I/O 和后续层的大小。 容器依赖技术 在宿主机上看，一个容器其实就是宿主机的一个进程，但是这个进程是和宿主机存在隔离的。容器内的进程只能看到自己 namespace 的“世界”，与宿主机上的其他进程互相无感知。主要是使用了 Linux 内核底层的 namespace 和 cgroup 技术。 Namespace 资源隔离 namespace 是 Linux 提供的一种由内核实现的隔离技术。是在 Unix 的 chroot 系统调用（通过修改根目录把用户监禁在一个特定目录下）的基础上，实现了六种 namespace 隔离机制。 Namespace 系统调用参数 隔离内容 UTS CLONE_NEWUTS 主机名和域名 IPC CLONE_NEWIPC 共享内存、信号量、消息队列 PID CLONE_NEWPID 进程号 Network CLONE_NEWNET 网络设备、网络协议栈、端口 Mount CLONE_NEWNS 文件系统挂载点 User CLONE_NEWUSER 用户和用户组 Cgroup 资源限制 Cgroup（Control Group）是 Linux 内核的一个功能，用来限制、控制与分离一个进程组群的资源（例如 CPU、内存、磁盘 IO 等）。Cgroup 可以为系统中运行的进程分配资源，拒绝进程访问某些资源，还提供 Cgroup 配置的监控。 Cgroup 的主要作用有： Resource limitation: 限制资源使用，例如 CPU、内存、磁盘。 Prioritization: 优先级控制，为进程组分配特定的 CPU（多核） 或者磁盘 IO 吞吐。 Accounting: 统计资源使用量，CPU 使用时间、内存使用量等，按量计费非常有用。 Control: 进程组控制，挂起或者恢复执行进程。 Cgroup 子系统： Cgroup 子系统 作用 blkio 为块设备设定 IO 限制，例如磁盘 cpu 使用调度程序提供对 CPU 的 Cgroup 任务访问 cpuacct 自动生成 cgroup 中任务所使用的 CPU 报告 cpuset 给 Cgroup 中的任务分配独立 CPU（多核）和内存节点 memory 为 Cgroup 任务提供对 Memory 的限制 freezer 暂停和恢复 Cgroup 任务 devices 允许或拒绝 Cgroup 任务对设备的访问 net_cls 标记网络数据包，可允许 Linux 流量控制程序识别从具体 Cgroup 中生成的数据包 net_prio 设计网络流量的优先级 hugetlb 主要针对于 HugeTLB 系统进行限制，这是一个大页文件系统 镜像和容器的操作 配置镜像加速 每一个开通了阿里云容器镜像服务的用户，都会有一个镜像加速地址。 [root@docker ~]# sudo tee /etc/docker/daemon.json 下载镜像 Docker Hub 是 Docker 官方提供的公共镜像仓库，使用 docker pull 命令默认就是从这个仓库拉取镜像。拉取镜像的命令格式如下： [root@docker ~]# docker pull [OPTIONS] NAME[:TAG|@DIGEST] 镜像名称后面可以跟上标签或者镜像摘要，不写的话 Docker 默认拉取 latest（最新版本）。如果要拉取某一个镜像，但是不知道仓库中是否有这个镜像，可以使用搜索查看，--filter 是根据条件对搜索结果进行过滤。 [root@docker ~]# docker search --filter STARS=100 nginx NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 15928 [OK] jwilder/nginx-proxy Automated Nginx reverse proxy for docker con… 2101 [OK] richarvey/nginx-php-fpm Container running Nginx + PHP-FPM capable of… 820 [OK] jc21/nginx-proxy-manager Docker container for managing Nginx proxy ho… 288 linuxserver/nginx An Nginx container, brought to you by LinuxS… 160 tiangolo/nginx-rtmp Docker image with Nginx using the nginx-rtmp… 147 [OK] jlesage/nginx-proxy-manager Docker container for Nginx Proxy Manager 145 [OK] alfg/nginx-rtmp NGINX, nginx-rtmp-module and FFmpeg from sou… 111 [OK] [root@docker ~]# 这里下载的是 ubuntu:20.04 的镜像，library/ubuntu 指的用户名/仓库名，library 则是官方默认的名字。 [root@docker ~]# docker pull ubuntu:20.04 20.04: Pulling from library/ubuntu Digest: sha256:626ffe58f6e7566e00254b638eb7e0f3b11d4da9675088f4781a50ae288f3322 Status: Downloaded newer image for ubuntu:20.04 docker.io/library/ubuntu:20.04 [root@docker ~]# 查看镜像 我们可以列出下载的镜像信息，信息包含仓库名、标签、镜像 ID、创建时间以及所占的空间。 [root@docker ~]# docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest f652ca386ed1 5 days ago 141MB ubuntu 20.04 ba6acccedd29 7 weeks ago 72.8MB ubuntu latest ba6acccedd29 7 weeks ago 72.8MB ubuntu 18.04 5a214d77f5d7 2 months ago 63.1MB [root@docker ~]# 启动容器 下载好镜像之后，我们就能以镜像为基础去启动一个容器了。这里以 ubuntu:20.04 的镜像启动一个容器，并在容器中执行 date 命令后退出并删除容器，屏幕打印的信息与我们平时执行 date 命令一样，但是这个信息是执行容器内的 date 命令打印出来的。 [root@docker ~]# docker run --rm ubuntu:20.04 /bin/date Wed Dec 8 09:34:46 UTC 2021 [root@docker ~]# 上面是以非交互式启动的容器，容器还可以交互式启动。其中，-t 是分配一个伪终端并绑定到容器的标准输入上，-i 是以交互式启动，就是让容器的标准输入保持打开。在交互模式下，用户创建容器之后自动进入到容器环境了，我们可以看到 ubuntu 容器也拥有自己的根目录结构。 [root@docker ~]# docker run -it ubuntu:20.04 /bin/bash root@cc0a1a750941:/# ls bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr var [root@docker ~]# 但是更多时候，我们的容器都是后台运行，就是不把容器内的执行结果输出到宿主机上，通过 -d 参数实现。后台运行启动的容器，会返回一个唯一的容器 ID。 [root@docker ~]# docker run -d ubuntu:20.04 /bin/sh -c \"while true; do echo 'hello docker' >> /var/log/docker.log; sleep 1; done\" 20b03a8a8d7463efa2ad2cd045db654be27a83f4f499a9fb8df83436d2cfdc53 [root@docker ~]# 可以通过 docker container ls 命令来查看运行容器的信息，加上 -a 参数是显示所有容器。 [root@docker ~]# docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 26 minutes ago Up 26 minutes flamboyant_booth [root@docker ~]# 进入容器 我们使用 -d 参数创建的容器，默认会丢到后台去运行，但是很多时候我们需要进入到一个后台运行的容器进行某些操作，这时候就需要使用到 exec 命令了。 [root@docker ~]# docker exec -it flamboyant_booth bash root@20b03a8a8d74:/# ls bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr var root@20b03a8a8d74:/# 如果仅使用 -i 参数，由于没有 -t 参数分配的伪终端，进入容器后没有 Linux 命令提示符，但是仍然可以执行命令并返回结果。所以一般 -it 参数一起使用。 [root@docker ~]# docker exec -i flamboyant_booth bash date Mon Dec 13 01:24:06 UTC 2021 pwd / 我们使用 exec 命令进入容器，如果这个标准输入 exit 时，并不会导致容器停止。容器还在正常运行，只是这个标准输入断开了。可以按下 Ctrl + d 或者在容器命令行执行 exit 退出容器。 [root@docker ~]# docker exec -it flamboyant_booth bash root@20b03a8a8d74:/# date Mon Dec 13 01:31:49 UTC 2021 root@20b03a8a8d74:/# exit 停止容器 我们可以手动执行 docker container stop 命令停止容器。当然，容器内的应用程序运行结束时，容器也会自动退出。退出状态的容器需要使用 docker container ls -a 命令才能看到。我们还可以使用 docker container start 命令来启动容器，docker container restart 命令是将一个容器停止再重新启动。 [root@docker ~]# docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 2 days ago Up 2 days flamboyant_booth [root@docker ~]# docker container stop flamboyant_booth flamboyant_booth [root@docker ~]# docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 2 days ago Exited (137) 2 seconds ago flamboyant_booth [root@docker ~]# 删除容器 容器是有生命周期的。因为运行结束而进入退出状态的容器，或者因为发布版本而被替换的容器，这些容器都不处于运行的状态，我们可以使用 docker container rm 命令来删除这些容器。如果要删除一个运行状态的容器，可以使用 -f 参数，docker 会发送 SIGKILL 信号停止容器进程并删除容器。 [root@docker ~]# docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6ca17aec7f97 ubuntu:20.04 \"bash date\" 28 hours ago Exited (126) 28 hours ago condescending_bouman 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 3 days ago Up 29 hours flamboyant_booth [root@docker ~]# docker container rm 6ca17aec7f97 6ca17aec7f97 [root@docker ~]# docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 4 days ago Up 30 hours flamboyant_booth [root@docker ~]# 如果需要删除所有退出状态的容器，可以使用 docker container prune 命令一次性清理。 [root@docker ~]# docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c38b5ecba22c ubuntu:20.04 \"/bin/bash -c pwd\" 5 seconds ago Exited (0) 3 seconds ago gallant_torvalds c69212a7de81 ubuntu:20.04 \"/bin/bash -c date\" 19 seconds ago Exited (0) 17 seconds ago quizzical_raman 20b03a8a8d74 ubuntu:20.04 \"/bin/sh -c 'while t…\" 4 days ago Up 30 hours flamboyant_booth [root@docker ~]# docker container prune WARNING! This will remove all stopped containers. Are you sure you want to continue? [y/N] y Deleted Containers: c38b5ecba22c24c15789a53b89a687ec7ecf918e2b4e42d3548d5c101cac54e7 c69212a7de811575b2dc7994059ba3bf18479312d18273c1e5f63e0685860696 Total reclaimed space: 0B [root@docker ~]# 删除镜像 删除本地的镜像使用 docker image rm 命令，或者 docker rmi 命令。其中，可以使用镜像 ID、镜像名、摘要来指定要删除的镜像。下面我们删除 redis 镜像： [root@docker ~]# docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE redis latest aea9b698d7d1 11 days ago 113MB nginx latest f652ca386ed1 11 days ago 141MB centos latest 5d0da3dc9764 2 months ago 231MB [root@docker ~]# docker image rm redis:latest Untagged: redis:latest Untagged: redis@sha256:2f502d27c3e9b54295f1c591b3970340d02f8a5824402c8179dcd20d4076b796 Deleted: sha256:aea9b698d7d1d2fb22fe74868e27e767334b2cc629a8c6f9db8cc1747ba299fd Deleted: sha256:beb6c508926e807f60b6a3816068ee3e2cece7654abaff731e4a26bcfebe04d8 Deleted: sha256:a5b5ed3d7c997ffd7c58cd52569d8095a7a3729412746569cdbda0dfdd228d1f Deleted: sha256:ee76d3703ec1ab8abc11858117233a3ac8c7c5e37682f21a0c298ad0dc09a9fe Deleted: sha256:60abc26bc7704070b2977b748ac0fd4ca94b818ed4ba1ef59ca8803e95920161 Deleted: sha256:6a2f1dcfa7455f60a810bb7c4786d62029348f64c4fcff81c48f8625cf0d995a [root@docker ~]# docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest f652ca386ed1 11 days ago 141MB centos latest 5d0da3dc9764 2 months ago 231MB [root@docker ~]# 备份迁移 save 和 load docker save 命令可以保存一个或多个镜像，例如将本地的 centos 镜像和 ubuntu 镜像保存到 image.tar 文件中。 [root@docker ~]# docker save -o images.tar centos:latest ubuntu:latest [root@docker ~]# ll -sh images.tar 300M -rw------- 1 root root 300M Dec 15 11:40 images.tar [root@docker ~]# docker load 命令用于将由 docker save 命令打包生成的 tar 文件载入到本地镜像。 [root@docker ~]# docker load -i images.tar 74ddd0ec08fa: Loading layer [==================================================>] 238.6MB/238.6MB Loaded image: centos:latest Loaded image: ubuntu:latest [root@docker ~]# export 和 import docker export 命令可以保存 container 文件系统，例如将运行中的容器保存为 ubuntu.tar 文件。 [root@docker ~]# docker export 20b03a8a8d74 -o ubuntu.tar docker import 命令用于将由 docker export 命令打包生成的 tar 文件载入到本地镜像。 [root@docker ~]# docker import ubuntu.tar ubuntu:1.0 sha256:bcead6020c0ca44be9d380d29e6cf1e5ed304e855f93858e897374ca7ec84734 [root@docker ~]# docker save 命令保存的是镜像，docker export 命令保存的是容器。docker load 命令用来载入镜像包文件，docker import 命令用来载入容器包文件，但是两者都是将包文件载入到本地镜像。在载入镜像时，docker load 命令不能对镜像重命名，而 docker import 命令可以对镜像指定名称。 容器日志 后台运行的容器，应用程序的日志并不会打印出来。如果没有配置数据持久化，日志文件是存放在容器内的。但是我们不需要进入容器才能查看日志，我们可以使用 docker logs 命令实现。 [root@docker ~]# docker logs -f nginx /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/ /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh 10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf 10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh /docker-entrypoint.sh: Configuration complete; ready for start up 2021/12/16 07:00:00 [notice] 1#1: using the \"epoll\" event method 2021/12/16 07:00:00 [notice] 1#1: nginx/1.20.2 2021/12/16 07:00:00 [notice] 1#1: built by gcc 10.2.1 20210110 (Debian 10.2.1-6) 2021/12/16 07:00:00 [notice] 1#1: OS: Linux 5.4.0-91-generic 2021/12/16 07:00:00 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576 2021/12/16 07:00:00 [notice] 1#1: start worker processes 2021/12/16 07:00:00 [notice] 1#1: start worker process 31 2021/12/16 07:00:00 [notice] 1#1: start worker process 32 资源控制 我们在启动容器的时候，可以控制容器对资源的使用。例如限制容器最多使用 500M 物理内存，物理内存 + 交换分区使用的总大小限制在 800M，并且禁用 OOM Killer。 docker container run -d --name \"nginx\" --memory=\"500M\" --memory-swap=\"800M\" --oom-kill-disable nginx:1.20 [info] 说明 --memory-swap 是物理内存加交换分区的总使用量限制，并且 --memory-swap 必须比 --memory 设置的要大，--memory 允许的最小值为 6M。 如果 --memory 设置为500M，--memory-swap 不设置或者设置为 0，则容器可使用和 --memory 一样多的交换分区 ，那么容器的内存和交换分区一共可以使用 1G。 想要禁止容器使用交换分区，需要把 --memory 和 --memory-swap 设置为相同的值，因为 --memory-swap 是物理内存加交换分区的总使用量限制。 对于 CPU 资源的控制，例如主机有 2 个 CPU，限制容器最多使用 1.5 个 CPU。还可以使用 --cpus=\".5\" 参数让容器总是使用 50% 的 CPU。 docker container run -d --name \"nginx\" --cpus=\"1.5\" nginx:1.20 我们还可以限制容器使用特定的 CPU 或特定的核，例如主机有多个 CPU，则容器可以使用 CPU 编号（从 0 开始）加逗号分隔符来描述容器具体使用那个 CPU。 [root@docker ~]# docker container run -d --name \"nginx\" --cpuset-cpus=\"0,1\" nginx:1.20 a242e04bff0c10ed6388001119e0d08d4f85a8c075b6f0e0e1c81fa87b195944 [root@docker ~]# docker inspect nginx | grep CpusetCpus \"CpusetCpus\": \"0,1\", [root@docker ~]# "},"worknote/docker/data-persistence.html":{"url":"worknote/docker/data-persistence.html","title":"数据持久化","keywords":"","body":"数据持久化 由于容器的镜像分层机制，我们在容器里面创建文件或者修改文件，结果都会保存在容器的可读写层中，一旦容器被销毁，那么这个可读写层也会随着容器销毁而消失。而且当一个容器需要和其他容器的读写层进行数据交互时，也会显得非常困难。于是在容器数据持久化方面，Docker 为我们提供了三种持久化的方式。 bind mount 持久化方式 bind mount 本质上是将宿主机上的文件或目录挂载到容器中供容器使用，文件或目录由其主机上的绝对路径或者相对路径引用，这跟 volume 持久化方式由 Docker 统一管理存储目录不一样。如果宿主机上目录不存在则会自动创建，但不能创建文件。如果容器目录非空，则容器目录现有内容会被宿主机目录内容所隐藏，容器内的数据要卸除挂载后才会恢复。 将宿主机上的 /data/nginx 目录挂载到容器中的 /usr/share/nginx/html 目录，容器内的 nginx 默认页面会被隐藏，即使宿主机的目录为空。 docker container run -itd --name \"nginx\" --volume /data/nginx:/usr/share/nginx/html nginx:1.20 docker container run -itd --name \"nginx\" --mount type=bind,source=/data/nginx,target=/usr/share/nginx/html nginx:1.20 [info] 说明 关于 --volume（-v） 参数和 --mount 参数的区别，最初 --volume 和 -v 参数用于独立容器，--mount 用于集群服务。--volume 和 -v 参数如果源路径不存在则会自动创建，--mount 源路径不存在则会报错。但是在 Docker 17.06 开始，官方推荐使用 --mount 去进行挂载，因为只有 --mount 支持指定卷驱动类型，同时语法更明确和易于理解，新用户都应使用 --mount 参数。 bind mount 无法使用 docker cli 命令直接管理，我们可以在容器的详细信息里看到挂载的内容。 [root@docker ~]# docker container inspect nginx | grep Mounts \"Mounts\": [ { \"Type\": \"bind\", \"Source\": \"/data/nginx\", \"Destination\": \"/usr/share/nginx/html\", \"Mode\": \"\", \"RW\": true, \"Propagation\": \"rprivate\" } ] bind mount 默认挂载开启了 rw 模式，如果容器只需要读访问权限，我们也可以将目录挂载为只读。 [root@docker ~]# docker container run -itd --name \"nginx\" --mount type=bind,source=/data/nginx,target=/usr/share/nginx/html,readonly nginx:1.20 a5c5f217e2557f58e83276ba9be9fd75e6127b34e918199d379b7734dac5806f [root@docker ~]# docker exec -it nginx /bin/bash root@a5c5f217e255:/# echo 'test' > /usr/share/nginx/html/index.html bash: /usr/share/nginx/html/index.html: Read-only file system root@a5c5f217e255:/# volume 持久化方式 volume 由 Docker 负责管理，可以使用 docker volume create 命令创建 volume。Docker 创建的 volume 本质上还是宿主机文件系统中的一个目录（可以使用 docker volume inspect 命令查看目录路径），一个 volume 可以供一个或多个容器一同使用，即使没有容器使用此 volume 它也不会自动删除，除非用户明确删除它。如果是用户使用命令创建的则需要指定名称，如果是 container 和 service 启动的隐式创建，Docker 则会为它分配一个宿主机范围内唯一的名字。通过使用第三方提供的 volume driver，用户可以将数据持久化到远程主机或者云存储中，也就是说存储空间可以不由宿主机提供。 管理 volume 资源使用 docker volume 命令。 # 创建 volume docker volume create nginx_volumes # 查看 volume docker volume ls # 查看 volume 信息 docker volume inspect nginx_volumes # 删除 volumes docker volume rm nginx_volumes 可以先创建好 volume，启动容器时再指定 volume，也可以在启动容器时直接指定 volume，如果不存在则自动创建 volume。 docker container run -itd --name \"nginx\" -p 80:80 --volume nginx_volumes:/usr/share/nginx/html nginx:1.20 docker container run -itd --name \"nginx\" -p 80:80 --mount source=nginx_volumes,target=/usr/share/nginx/html nginx:1.20 [info] 说明 关于 --volume（-v） 参数和 --mount 参数的区别，最初 --volume 和 -v 参数用于独立容器，--mount 用于集群服务。--volume 和 -v 参数如果源路径不存在则会自动创建，--mount 源路径不存在则会报错。但是在 Docker 17.06 开始，官方推荐使用 --mount 去进行挂载，因为只有 --mount 支持指定卷驱动类型，同时语法更明确和易于理解，新用户都应使用 --mount 参数。 我们能够看到容器内的文件已经被映射到宿主机的 volume 目录上了。 [root@docker ~]# docker volume inspect nginx_volumes [ { \"CreatedAt\": \"2021-12-21T11:12:19+08:00\", \"Driver\": \"local\", \"Labels\": null, \"Mountpoint\": \"/var/lib/docker/volumes/nginx_volumes/_data\", \"Name\": \"nginx_volumes\", \"Options\": null, \"Scope\": \"local\" } ] [root@docker ~]# ll /var/lib/docker/volumes/nginx_volumes/_data total 16 drwxr-xr-x 2 root root 4096 Dec 21 11:12 ./ drwx-----x 3 root root 4096 Dec 21 11:12 ../ -rw-r--r-- 1 root root 494 Nov 16 22:44 50x.html -rw-r--r-- 1 root root 612 Nov 16 22:44 index.html [root@docker ~]# volume 持久化方式宿主机和容器的文件映射，当宿主机 volume 目录非空时以宿主机的文件为准，当宿主机 volume 目录为空时将会把容器内的数据复制到宿主机后再以宿主机为准。 tmpfs 持久化方式 tmpfs 挂载是临时的，并且仅保留在宿主机的内存中。当容器停止时，tmpfs 挂载将被删除，而且 tmpfs 挂载不能在多个容器之间共享。我们可以使用 --tmpfs 或者 --mount 参数创建 tmpfs 挂载。 docker container run -itd --name \"nginx\" -p 80:80 --tmpfs /usr/share/nginx/html nginx:1.20 docker container run -itd --name \"nginx\" -p 80:80 --mount type=tmpfs,destination=/usr/share/nginx/html nginx:1.20 [info] 说明 关于 --tmpfs 和 --mount 参数，--tmpfs 参数不允许指定任何可配置选项，需要可配置选项必须使用 --mount。同时 --mount 参数的语法更明确和易于理解，推荐使用 --mount 参数。 通过 tmpfs 挂载还有两个可选参数，但是在指定可选参数时，必须使用 --mount 标志，因为 --tmpfs 不支持。 可选参数 作用 tmpfs-size 指定 tmpfs 挂载的大小，以字节为单位，默认无限制。 tmpfs-mode 八进制中 tmpfs 的文件模式，默认为 1777。 "},"worknote/docker/network.html":{"url":"worknote/docker/network.html","title":"网络","keywords":"","body":"网络概述 Docker 容器和服务如此强大的原因之一是可以将它们连接到一起，或将它们连接到其他的非 Docker 工作负载上。Docker 容器甚至不需要知道它们部署在 Docker 上，或者它们的对端是否也是 Docker 工作负载。无论你的 Docker 主机是运行在 Linux 还是 Windows 又或者两者都有，你都可以使用 Docker 以平台无关的方式管理它们。 Docker 的网络子系统是使用驱动程序插入的。默认情况下存在多个驱动程序，它们一起提供核心网络功能，可以使用 docker network ls 命令查看默认存在的驱动程序。 [root@docker ~]# docker network ls NETWORK ID NAME DRIVER SCOPE ef0a155fdde4 bridge bridge local c4fa50b15f43 host host local 6a2c541b2e82 none null local [root@docker ~]# Docker 常用的网络类型 Bridge 默认的网络驱动程序，如果你创建容器时不指定网络驱动程序，这就是你正在创建容器的网络类型。当你的应用程序在需要通信的独立容器中运行时，通常会使用桥接网络。当启动 docker.service 服务之后，Docker 会默认创建一个名为 docker0 的虚拟网桥，所有没有指定网络驱动程序的容器都会被加入到这个网桥中。虚拟网桥就像一个物理交换机，这样网桥内的容器都连接到了一个二层网络中。Docker 会在宿主机上创建一对 veth pair 虚拟网卡设备，这一对 veth pair 虚拟网卡设备的两端分别对应容器内部的 eth0 网卡和宿主机上类似 vethxxxxxx 的虚拟网卡，可以使用 brctl show 命令查看宿主机上已经加入到 docker0 网桥下的虚拟网卡设备。虚拟网卡设备连接到 docker0 网桥后， docker0 网桥会从子网分配一个 ip 给容器，并且设置 docker0 网桥的 ip 地址为容器的默认网关。 查看 docker0 网桥上的接口信息。 [root@docker ~]# brctl show bridge name bridge id STP enabled interfaces docker0 8000.02429451522b no veth00b57b0 veth0bf3f4c veth59d431c [root@docker ~]# 创建容器时不指定网络驱动程序，默认就是 bridge 网络类型，但是我们还可以创建自定义的 bridge 网络类型。 [root@docker ~]# docker network create app 0a4fd2947aa9dcf12786709bcce856421b33c7b2fdcc6175691be4d42cec0d07 [root@docker ~]# docker network ls NETWORK ID NAME DRIVER SCOPE 0a4fd2947aa9 app bridge local ef0a155fdde4 bridge bridge local c4fa50b15f43 host host local 6a2c541b2e82 none null local 创建两个容器并把它们加入到自定义的 app 网络，-d 参数可以指定创建 bridge 或者 overlay 网络类型。 [root@docker ~]# docker container run -itd --name \"app1\" --network app busybox f46be3d1f5ead467e2f7e8df05f72d73e6d3a3b3aaa2d34a49a5879a99fa53d1 [root@docker ~]# docker container run -itd --name \"app2\" --network app busybox 7c2b79e36244d79ad8e184d916ddba23fbb026b1bdc6e3c333d7110f705695de [root@docker ~]# 随着 Docker 网络的完善，建议大家将容器加入自定义的网络来进行连接。通过在容器内使用 ping 命令来证明容器网络的互通。 [root@docker ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 7c2b79e36244 busybox \"sh\" 4 minutes ago Up 4 minutes app2 f46be3d1f5ea busybox \"sh\" 5 minutes ago Up 5 minutes app1 [root@docker ~]# docker exec -it app1 sh / # ping -c 3 app2 PING app2 (172.19.0.3): 56 data bytes 64 bytes from 172.19.0.3: seq=0 ttl=64 time=0.071 ms 64 bytes from 172.19.0.3: seq=1 ttl=64 time=0.111 ms 64 bytes from 172.19.0.3: seq=2 ttl=64 time=0.107 ms --- app2 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.071/0.096/0.111 ms / # Host 如果容器使用 host 网络类型，则容器的网络堆栈不会与 Docker 主机隔离（容器共享主机的网络命名空间）。并且容器不会分配自己的 ip 地址，而是使用宿主机的 ip 和端口，容器的文件系统和进程等还是和宿主机隔离的。 [info] 说明 由于容器使用 host 网络模式没有主机的 ip 地址，端口映射不会生效，-p 和 -P 参数都会被忽略并且产生一个错误警告：WARNING: Published ports are discarded when using host network mode 创建一个 nginx 容器，可以看到宿主机上的 80 端口被占用了。 [root@docker ~]# docker container run -itd --name \"nginx\" --network=host nginx:1.20 1a13399898af4d66758a0ec10f4f8e3d89cd21b15186c6cea9c161cca0d82b87 [root@docker ~]# ps -ef | grep nginx root 260249 260228 1 14:38 pts/0 00:00:00 nginx: master process nginx -g daemon off; systemd+ 260308 260249 0 14:38 pts/0 00:00:00 nginx: worker process systemd+ 260309 260249 0 14:38 pts/0 00:00:00 nginx: worker process root 260313 112281 0 14:38 pts/1 00:00:00 grep --color=auto nginx [root@docker ~]# netstat -lntup | grep 80 tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 260249/nginx: maste tcp6 0 0 :::80 :::* LISTEN 260249/nginx: maste [root@docker ~]# 使用 ps 命令可以看到 nginx 进程的父进程是 containerd-shim，这也证实了 nginx 进程是容器内的。 [root@docker ~]# ps -axf | grep containerd-shim -A 1 260701 pts/1 S+ 0:00 \\_ grep --color=auto containerd-shim -A 1 260228 ? Sl 0:00 /usr/bin/containerd-shim-runc-v2 -namespace moby -id 1a13399898af4d66758a0ec10f4f8e3d89cd21b15186c6cea9c161cca0d82b87 -address /run/containerd/containerd.sock 260249 pts/0 Ss+ 0:00 \\_ nginx: master process nginx -g daemon off; [root@docker ~]# 进入容器内部查看网卡信息会看到宿主机的网卡，这是因为容器共享了宿主机网络堆栈。 [root@docker ~]# docker container run -itd --name \"busybox\" --network=host busybox ad4634aa3d4d52b7ccd086359fe20e0fe5abae5527ac6a78064a0c308dde8f09 [root@docker ~]# docker exec -it busybox /bin/sh / # ip a 1: lo: mtu 65536 qdisc noqueue qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: mtu 1500 qdisc fq_codel qlen 1000 link/ether 00:0c:29:f9:6d:2f brd ff:ff:ff:ff:ff:ff inet 10.10.110.31/24 brd 10.10.110.255 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fef9:6d2f/64 scope link valid_lft forever preferred_lft forever 3: docker0: mtu 1500 qdisc noqueue link/ether 02:42:94:51:52:2b brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:94ff:fe51:522b/64 scope link valid_lft forever preferred_lft forever 15: br-0a4fd2947aa9: mtu 1500 qdisc noqueue link/ether 02:42:f8:6b:79:d0 brd ff:ff:ff:ff:ff:ff inet 172.19.0.1/16 brd 172.19.255.255 scope global br-0a4fd2947aa9 valid_lft forever preferred_lft forever inet6 fe80::42:f8ff:fe6b:79d0/64 scope link valid_lft forever preferred_lft forever / # Container 创建新容器时指定和一个已存在的容器共享一个网络命名空间，这与宿主机的网络命名空间无关。新容器不会创建自己的网卡和 ip 地址，而是共享已存在容器的网卡、ip、端口等。但是除了网络环境之外，其他的资源例如文件系统和进程等还是隔离的，这就是 container 网络类型。 创建一个 busybox 容器，用于给其他容器共享网络命名空间。 [root@docker ~]# docker container run -itd --name 'busybox' busybox 471accba2c3fef25da6a617417b8c84fb38c860aa49a4e9287dcbacd111b73f2 [root@docker ~]# 创建新的容器，指定新的容器共享前面创建的 busybox 容器的网络命名空间。 [root@docker ~]# docker container run -itd --network=container:busybox --name 'busybox-1' busybox a1e6999c17e35a7ea32c9886f3f708985a50a8c1b8b1bc36b790b10a9db98d59 [root@docker ~]# docker container run -itd --network=container:busybox --name 'busybox-2' busybox 867915a468489dd356efbe0c1ea22430df3dd026b74267caa7576b3eec298f67 [root@docker ~]# 共享 busybox 容器网络命名空间的 busybox-1 和 busybox-2 容器，ip 的值为 \\，其实容器内的 ip 地址是和 busybox 容器一样的。 [root@docker ~]# docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' busybox 172.17.0.2 [root@docker ~]# docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' busybox-1 [root@docker ~]# docker inspect --format='{{.NetworkSettings.Networks.bridge.IPAddress}}' busybox-2 [root@docker ~]# [root@docker ~]# docker container exec -it busybox ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:120 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:8672 (8.4 KiB) TX bytes:0 (0.0 B) [root@docker ~]# docker container exec -it busybox-1 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:120 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:8672 (8.4 KiB) TX bytes:0 (0.0 B) [root@docker ~]# docker container exec -it busybox-2 ifconfig eth0 eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:120 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:8672 (8.4 KiB) TX bytes:0 (0.0 B) [root@docker ~]# None none 模式会使容器拥有独立的 network namespace，就是不为 Docker 容器进行任何网络配置。容器内部只有 loopback 网络设备，这将网络创建的责任完全交给用户。Docker 开发者可以在这基础上做出更多的网络定制，这种方式可以实现更加灵活复杂的网络。 [root@docker ~]# docker container run -it --network=none --name 'busybox-3' busybox / # ifconfig -a lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # Flannel 实现 Docker 跨主机通信 flannel 是一种基于 overlay 网络的跨主机容器网络解决方案。就是将 TCP 数据包封装在另一种网络包里面进行路由转发和通信，flannel 是 CoreOS 团队针对 Kubernetes 设计的一个网络规划服务，让集群中的不同节点主机创建的容器都具有全集群唯一的虚拟 ip 地址，flannel 使用 go 语言编写。 flannel 为每个 host 分配一个 subnet，容器从这个 subnet 中分配 ip，这些 ip 可以在 host 间路由，容器间无需使用 nat 和端口映射即可实现跨主机通信。每个 subnet 都是从一个更大的 ip 池中划分的，flannel 会在每个主机上运行一个叫 flanneld 的 agent，其职责就是从 ip 池中分配 subnet。etcd 相当于一个数据库，flannel 使用 etcd 存放网络配置和已分配的 subnet、host、ip 等信息。这就是 flannel 的原理。 node software Operating System docker version 10.10.110.31(master) etcd、flannel、docker Ubuntu 20.04.3 LTS 19.03.12 10.10.110.32(slave) flannel、docker Ubuntu 20.04.3 LTS 19.03.12 master 节点安装 etcd root@docker1:~# apt install -y etcd root@docker1:~# vim /etc/default/etcd # 修改 master 默认的 127.0.0.1 地址以供 slave 节点访问 etcd ETCD_LISTEN_CLIENT_URLS=\"http://10.10.110.8:2379\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.10.110.8:2379\" root@docker1:~# systemctl restart etcd root@docker1:~# netstat -lntup | grep 2379 tcp 0 0 10.10.110.8:2379 0.0.0.0:* LISTEN 95984/etcd # 配置 etcd 的子网必须指定使用 V2 版本因为 flannel 目前不支持 etcd V3 版本 ETCDCTL_API=2 etcdctl --endpoints=\"http://10.10.110.8:2379\" set /atomic.io/network/config '{ \"Network\":\"172.17.0.0/16\", \"Backend\": {\"Type\": \"vxlan\"}} ' master 节点安装 flannel root@docker1:~# apt install -y flannel root@docker1:~# cat > /etc/systemd/system/flanneld.service slave 节点安装 flannel root@docker1:~# apt install -y flannel root@docker1:~# cat > /etc/systemd/system/flanneld.service 配置 docker 使用 flannel 网络，master 和 slave 都需要进行配置让 flannel 管理 docker 的网络 vim /lib/systemd/system/docker.service EnvironmentFile=/run/flannel/docker # 加载这个文件里面的变量,这个文件记录了 flannel 分配的子网信息 ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS # 使用上面文件的变量,启动容器时指定使用 flannel 分配的子网去配置容器的网络 iptables -P FORWARD ACCEPT systemctl daemon-reload systemctl restart flanneld.service systemctl restart docker.service 如果没有 /run/flannel/docker 这个文件，可以使用 mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker 命令生成。 # docker0 网卡的网段变成了 flannel.1 网卡的子网时，就说明配置成功 root@docker1:~# ip a s docker0 3: docker0: mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:6d:41:48:2a brd ff:ff:ff:ff:ff:ff inet 172.17.19.1/24 brd 172.17.19.255 scope global docker0 valid_lft forever preferred_lft forever root@docker1:~# ip a s flannel.1 4: flannel.1: mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 3e:f4:3d:f3:0d:4d brd ff:ff:ff:ff:ff:ff inet 172.17.19.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::3cf4:3dff:fef3:d4d/64 scope link valid_lft forever preferred_lft forever root@docker1:~# root@docker2:~# ip a s docker0 3: docker0: mtu 1500 qdisc noqueue state DOWN group default link/ether 02:42:4c:d8:64:b0 brd ff:ff:ff:ff:ff:ff inet 172.17.71.1/24 brd 172.17.71.255 scope global docker0 valid_lft forever preferred_lft forever root@docker2:~# ip a s flannel.1 4: flannel.1: mtu 1450 qdisc noqueue state UNKNOWN group default link/ether 12:0e:69:a9:8d:09 brd ff:ff:ff:ff:ff:ff inet 172.17.71.0/32 scope global flannel.1 valid_lft forever preferred_lft forever inet6 fe80::100e:69ff:fea9:8d09/64 scope link valid_lft forever preferred_lft forever root@docker2:~# 分别在 master 节点和 slave 节点创建容器进行验证。 root@docker1:~# docker run -it busybox sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:13:02 inet addr:172.17.19.2 Bcast:172.17.19.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:8 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:736 (736.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # ping -c3 172.17.71.2 PING 172.17.71.2 (172.17.71.2): 56 data bytes 64 bytes from 172.17.71.2: seq=0 ttl=62 time=0.720 ms 64 bytes from 172.17.71.2: seq=1 ttl=62 time=0.810 ms 64 bytes from 172.17.71.2: seq=2 ttl=62 time=0.731 ms --- 172.17.71.2 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.720/0.753/0.810 ms / # root@docker2:~# docker run -it busybox sh / # ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:47:02 inet addr:172.17.71.2 Bcast:172.17.71.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:9 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:806 (806.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) / # ping -c3 172.17.19.2 PING 172.17.19.2 (172.17.19.2): 56 data bytes 64 bytes from 172.17.19.2: seq=0 ttl=62 time=4.305 ms 64 bytes from 172.17.19.2: seq=1 ttl=62 time=0.858 ms 64 bytes from 172.17.19.2: seq=2 ttl=62 time=0.798 ms --- 172.17.19.2 ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.798/1.987/4.305 ms / # "},"worknote/docker/dockerfile.html":{"url":"worknote/docker/dockerfile.html","title":"Dockerfile","keywords":"","body":"Dockerfile 简介 Docker 可以通过读取 Dockerfile 中的指令自动生成镜像。Dockerfile 其实是一个文本，其中包含用户可以在命令行上调用所有命令来构建镜像，使用 Docker 构建镜像是一个连续执行指令自动构建的过程。 docker build 原理 docker build 命令根据 Dockerfile 文件和上下文去构建一个镜像。构建的上下文位置可以是指定的 PATH 或者 URL 。PATH 是本地文件系统上的目录，URL 是 Git 存储库的链接。 构建上下文是递归处理的，PATH 包含所有的子目录，URL 包含存储库及其子模块。这个例子展示了使用工作目录作为构建背景的构建命令 。 [root@docker ~]# docker build . Sending build context to Docker daemon 2.56kB 该构建由 Docker 守护进程去运行，构建过程首先把整个 PATH 的上下文（递归）发送给 Docker 守护进程。一般构建镜像都从一个空目录开始，并且添加 Dockerfile 和构建所需的文件。 [warning] 警告 不要使用 / 目录作为构建上下文的 PATH，因为这会将整个根目录的内容传输到 Docker 的守护进程。 Dockerfile 常用指令 Dockerfile 指令不区分大小写但通常约定为大写，Docker 顺序运行 Dockerfile 中的指令，Dockerfile 必须以 FROM 指令开头（FROM 之前只能有 ARG 指令），Docker 将以 # 开头的行视为注释。 FROM 指定一个基础镜像，在其基础上执行 Dockerfile 指令，得到我们需要的镜像。Dockerfile 必须要有 FROM 指令且以 FROM 指令开头。主机上没有指定的基础镜像时会去 Docker Hub 拉取，任何有效的镜像地址都是可以被指定为基础镜像的，只是使用公共存储库的镜像更为方便。 LABEL 以键值对的方式为镜像添加标签，指定镜像的元数据。一个镜像可以有多个标签，基础镜像或者父镜像的标签会被继承，如果已存在的标签具有不一样的值，则最新的值将会覆盖旧的值。 RUN 执行当前镜像顶层中的任何命令并提交结果。RUN 指令有两种格式： shell 格式：RUN 命令在 shell 中运行，默认情况下是由 Linux 的 /bin/sh -c 和 Windows 的 cmd /S /C 执行。 exec 格式：RUN [\"executable\", \"param1\", \"param2\"] exec 表单不会调用 shell 去执行，可以指定 shell 或者可执行文件，exec 表单被解析为一个 JSON 数组，必须使用双引号。 Dockerfile 中的每个指令都会新建一层镜像，遵循 Dockerfile 的最佳实践，我们应该减少镜像层数避免镜像过于臃肿，在使用 RUN 指令时我们可以使用反斜杠 \\ 把多个命令写成一行： RUN /bin/bash -c 'source $HOME/.bashrc; \\ echo $HOME' COPY 复制本地文件或目录并添加到容器文件系统的路径中。可以指定多个 资源，但是文件或目录的路径是相对于构建上下文的 PATH 开始的， 路径必须位于构建的上下文中，我们不能 COPY ../something /something，因为 Docker 构建的第一步就是把整个 PATH 的上下文（递归）发送给 Docker 守护进程。 ADD COPY 只支持简单的复制文件或目录，而 ADD 支持复制文件、目录或远程 URL 文件。ADD 不同于 COPY 的是还支持复制 tar 归档文件时自动解压缩。COPY 和 ADD 在复制目录时都不复制目录本身，只复制目录的内容。 ENV 以键值对的形式指定环境变量，该键值对会存在于构建阶段中所有后续指令的环境中，并在容器运行时保持。 USER 指定运行镜像时的用户名（UID）或用户组（GID），以及 Dockerfile 中跟随它的任何 RUN、 CMD 和 ENTRYPOINT 指令。 WORKDIR 为 Dockerfile 中的任何 CMD、 ENTRYPOINT、 COPY 和 ADD 指令设置工作目录。如果 WORKDIR 路径不存在则会自动创建，即使 Dockerfile 后续的指令没有使用它。 EXPOSE 指定容器运行时监听的网络端口，可以指定端口是监听 TCP 还是 UDP，如果未指定协议则默认是 TCP。 VOLUME 声明容器中的目录作为匿名卷，自动挂载到本地的 /var/lib/docker/volumes/ 目录（根据 Docker 的版本会有所不同 ） [info] 说明 VOLUME 只能挂载到本地的 /var/lib/docker/volumes/ 目录，而 docker run -v 命令可以指定挂载到本地的具体目录，VOLUME 不能指定挂载到本地的目录是因为这样会破坏容器的可移植性，毕竟每个人映射的本地目录不同。VOLUME 的设计只是为了在启动容器时 docker run -v 没有指定也能成功启动，而且数据不会被写到容器中。如果 VOLUME 声明了容器中的目录作为匿名卷，但是 docker run -v 启动容器时指定了不一样的目录，这时以 docker run -v 为准。 CMD CMD 指令有三种形式： CMD [\"executable\",\"param1\",\"param2\"] （exec 表单） CMD [\"param1\",\"param2\"] （作为 ENTRYPOINT 的默认参数） CMD command param1 param2 （shell 形式表单） CMD 主要为容器启动提供默认值。默认值可以是可执行文件加参数，也可以忽略可执行文件而提供执行的参数，但是这时需要指定 ENTRYPOINT 指令。Dockerfile 中只能有一条 CMD 指令。如果有多个 CMD 指令，则只有最后一个 CMD 指令生效。如果启动容器时指定了 docker run 的参数，那么 CMD 中指定的默认参数则被覆盖。 ENTRYPOINT ENTRYPOINT 指令有两种形式： ENTRYPOINT [\"executable\", \"param1\", \"param2\"] （exec 表单） ENTRYPOINT command param1 param2 （shell 形式） ENTRYPOINT 把容器作为一个可执行文件去运行。只有 Dockerfile 中的最后一条 ENTRYPOINT 指令才有效。同时定义了 CMD 和 ENTRYPOINT 则 CMD 将作为 ENTRYPOINT 的默认参数。 [info] 说明 当 docker run 启动容器时没有指定参数，CMD 将作为 ENTRYPOINT 的默认参数。当 docker run 启动容器时指定参数 'hello world' 则 CMD 的参数会 'hello world' 被覆盖，而执行 ENTRYPOINT + 'hello world' ，ENTRYPOINT 指令比 CMD 指令优先级更高。在执行 docker run 命令时指定 --entrypoint 参数可以覆盖 dockerfile 中的 ENTRYPOINT 指令。 构建镜像 创建 Dockerfile 文件以及构建所需的脚本。 FROM openjdk:18-jdk-oraclelinux8 RUN microdnf install findutils git ARG MAVEN_VERSION=3.8.6 ARG USER_HOME_DIR=\"/root\" ARG SHA=f790857f3b1f90ae8d16281f902c689e4f136ebe584aba45e4b1fa66c80cba826d3e0e52fdd04ed44b4c66f6d3fe3584a057c26dfcac544a60b301e6d0f91c26 ARG BASE_URL=https://apache.osuosl.org/maven/maven-3/${MAVEN_VERSION}/binaries RUN mkdir -p /usr/share/maven /usr/share/maven/ref \\ && curl -fsSL -o /tmp/apache-maven.tar.gz ${BASE_URL}/apache-maven-${MAVEN_VERSION}-bin.tar.gz \\ && echo \"${SHA} /tmp/apache-maven.tar.gz\" | sha512sum -c - \\ && tar -xzf /tmp/apache-maven.tar.gz -C /usr/share/maven --strip-components=1 \\ && rm -f /tmp/apache-maven.tar.gz \\ && ln -s /usr/share/maven/bin/mvn /usr/bin/mvn ENV MAVEN_HOME /usr/share/maven ENV MAVEN_CONFIG \"$USER_HOME_DIR/.m2\" COPY mvn-entrypoint.sh /usr/local/bin/mvn-entrypoint.sh COPY settings-docker.xml /usr/share/maven/ref/ ENTRYPOINT [\"/usr/local/bin/mvn-entrypoint.sh\"] CMD [\"mvn\"] 执行 docker build 命令去构建镜像 [root@docker ~/dockerfile]# docker build -t maven:3.8.6 . Sending build context to Docker daemon 6.144kB Step 1/13 : FROM openjdk:18-jdk-oraclelinux8 18-jdk-oraclelinux8: Pulling from library/openjdk f42059649055: Downloading [=============> ] 10.92MB/41.97MB 67a9c63ed3ba: Downloading [=========================================> ] 11.27MB/13.49MB 3719e81f67b1: Downloading [==> ] 11.02MB/188.1MB 18-jdk-oraclelinux8: Pulling from library/openjdk 5f160c0f6cac: Pull complete fb499df0377a: Pull complete 373b9e2b6c72: Pull complete Digest: sha256:f2c01a7c961c1f9147995b6415ced7d96d4c83ce01c4e49452303b9e6bce9b0f Status: Downloaded newer image for openjdk:18-jdk-oraclelinux8 ---> b83a192caadf Step 2/13 : RUN microdnf install findutils git ---> Running in e47de8dc1968 Downloading metadata... Downloading metadata... 查看并运行我们构建的镜像。 [root@docker ~/dockerfile]# docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE maven 3.8.6 0ec71ea6d286 About a minute ago 793MB openjdk 18-jdk-oraclelinux8 b83a192caadf 5 days ago 464MB [root@docker ~/dockerfile]# docker run -it maven:3.8.6 /bin/bash bash-4.4# mvn -v Apache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) Maven home: /usr/share/maven Java version: 18.0.1.1, vendor: Oracle Corporation, runtime: /usr/java/openjdk-18 Default locale: en, platform encoding: UTF-8 OS name: \"linux\", version: \"5.4.0-110-generic\", arch: \"amd64\", family: \"unix\" Dockerfile 最佳实践 Docker 通过从 Dockerfile 读取指令来自动构建镜像—— Dockerfile 是一个文本文件，其中包含构建给定图像所需的所有命令。Docker 镜像由只读层组成，每层代表一个 Dockerfile 指令，每一层都在前一层的基础上变化。当你运行一个镜像并生成一个容器时，你将在底层之上添加一个新的可写层（“容器层”）。对正在运行的容器所做的所有更改（如写入新文件、修改现有文件和删除文件）都写入到可写容器层。 创建临时的容器 Dockerfile 定义的镜像应该尽可能生成短暂的容器。所谓的“短暂”，就是可以被停止和销毁的容器，然后用极小的配置和工作量去替换。 建立上下文 当你执行 docker build 命令时，当前工作目录被称为构建上下文。Dockerfile 一般就在当前目录下，或者也可以使用 -f 指定具体位置，但无论 Dockerfile 位于何处，执行 docker build 命令的当前目录的所有文件和目录都会递归发送到 Docker 守护进程。因此我们在构建镜像时应该为构建上下文创建一个目录，并把构建镜像所需的文件放入其中。 通过 stdin 管道读入 Docker 构建上下文可以通过 stdin 管道发送 Dockerfile 来构建镜像。通过 stdin 管道传输 Dockerfile 对于执行一次性构建非常有用，无需将 Dockerfile 写入磁盘，而且使用来自 stdin 的 Dockerfile 构建映像时不发送构建上下文到 Docker 守护进程，这在构建镜像时不需要复制文件到镜像中的场景下可能很有用。 echo -e 'FROM busybox\\nRUN echo \"hello world\"' | docker build - 使用 .dockerignore 文件 要排除与构建无关的文件，支持与 .gitignore 文件相似的语法。 使用多阶段构建 使用多阶段构建可以大幅减少镜像的大小，而不是在减少中间层和文件上做努力。将编译代码和运行代码分两个阶段去做，第一个阶段把源代码编译为可执行的代码文件，第二个阶段引用第一个阶段得到的可执行代码文件然后运行代码。使用多阶段构建，最终运行代码的容器就不需要考虑构建代码所需的文件和编译环境。 # syntax=docker/dockerfile:1 FROM golang:1.16-alpine AS build # Install tools required for project # Run `docker build --no-cache .` to update dependencies RUN apk add --no-cache git RUN go get github.com/golang/dep/cmd/dep # List project dependencies with Gopkg.toml and Gopkg.lock # These layers are only re-built when Gopkg files are updated COPY Gopkg.lock Gopkg.toml /go/src/project/ WORKDIR /go/src/project/ # Install library dependencies RUN dep ensure -vendor-only # Copy the entire project and build it # This layer is rebuilt when a file changes in the project directory COPY . /go/src/project/ RUN go build -o /bin/project # This results in a single layer image FROM scratch COPY --from=build /bin/project /bin/project ENTRYPOINT [\"/bin/project\"] CMD [\"--help\"] 避免安装不必要的包 为了减少复杂性、依赖关系、文件大小和构建时间，避免安装额外的或不必要的包。 分离应用程序 每个容器应该只有一个关注点。将应用程序解耦为多个容器可以更容易的水平拓展和重用容器，例如一个 web 应用程序可能由三个独立的容器组成：web 应用程序、数据库、缓存，每个容器有自己独立的镜像，以分离的方式管理。虽然一个容器只运行一个进程是很好的经验法则，但不是硬性规定。规划好容器的应用程序，尽量保持容器的干净和模块化。如果容器互相依赖，可以使用 Docker 容器网络来进行通信。 最小化图层数 在旧版本的 Docker 中，最小化镜像中的层数以确保性能是非常重要的。为了减少这个限制，现在的版本已经得到改善： 只有 RUN，COPY，ADD 会创建镜像层。其他指令创建临时中间镜像，并且不增加构建的大小。 尽可能的使用多阶段构建，并且只将构建得到的 artifacts 复制到最终的镜像。 排序多行参数 只要有可能，就对多行参数进行字母数字的排序（例如安装多个软件包时）。有助于避免包的重复，使安装列表更容易更新、阅读、审查。 RUN apt-get update && apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion \\ && rm -rf /var/lib/apt/lists/* 利用构建缓存 构建镜像时，Docker 按照指定的顺序逐步执行 Dockerfile 中的指令。在检查每条指令时，Docker 在其缓存中寻找可以重用的现有镜像，而不是创建新的(重复的)镜像。不想在构建过程中使用缓存可以指定 --no-cache=true选项。如果想在构建过程中使用缓存，那么了解到什么时候可以、什么时候不可以匹配到镜像就很重要了。Docker 遵循的基本规则如下： 从已经在缓存中的父镜像开始，下一条指令将与从该基础镜像派生的所有子镜像进行比较，以查看是否使用完全相同的指令构建了其中一个子镜像。否则，缓存将失效。 对于 ADD 和 COPY 指令，镜像中的文件也会被检查，每个文件计算出一个校验值。文件的修改时间和最后访问时间不会被纳入校验的范围。在缓存查找过程中，会将校验和现有镜像中的校验值进行比较。如果文件有任何改变，例如内容和元数据，则缓存失效。 "},"worknote/mysql/readme.html":{"url":"worknote/mysql/readme.html","title":"MySql","keywords":"","body":"MySql 欢迎来到 MySql 笔记页 "},"worknote/mysql/mysql-base.html":{"url":"worknote/mysql/mysql-base.html","title":"基础","keywords":"","body":"基础 这里是 MySQL 的基础知识内容。 "},"life/dream.html":{"url":"life/dream.html","title":"Dream","keywords":"","body":"Dream 曾经做了一个梦，独自一人走在荒凉的草原上。一望无际的草原让我绝望，我不知道为何会来到这里。这些都不重要，唯一让我在意的是清晨依附在草地上的露水，正是这些露水滋润着我干涸的灵魂，让我拖着疲惫的身体继续前行。 "},"GLOSSARY.html":{"url":"GLOSSARY.html","keywords":"","body":"jail 监狱的意思，这个技术的隔离思想来源于监狱的启发 "}}